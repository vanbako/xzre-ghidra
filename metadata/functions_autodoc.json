{
  "__tls_get_addr": "Same trap pattern as `lzma_check_init`: the compiled object ships a dummy `__tls_get_addr` that halts if invoked. The loader\nadjusts the GOT to point at `j_tls_get_addr` (and eventually the host's resolver); leaving this stub in place makes unexpected\nexecution obvious and prevents the payload from silently calling an incomplete resolver.",
  "_cpuid_gcc": "GCC-style CPUID shim that dispatches through the individual helper thunks for every supported leaf (basic, cache, topology,\nextended brand strings, etc.). Whatever leaf pointer it chooses has EAX/EBX/ECX/EDX copied into the provided outputs so callers\ndon\u2019t need inline assembly.",
  "_get_cpuid_modified": "Wrapper around `_cpuid_gcc` that first invokes `backdoor_entry` with the high-bit leaf to make sure the loader ran, checks the\nreturned maximum leaf, and only executes the requested CPUID if the CPU claims to support it. This is the exported symbol glibc\nbinds, so the loader\u2019s work is triggered before any sshd thread asks for cpuid data.",
  "backdoor_entry": "IFUNC resolver entry point. It increments a global invocation counter, calling `backdoor_init()` on the second pass so the\nloader can stage its hooks while glibc thinks it is still choosing a cpuid implementation. Regardless of setup, it finally\ndelegates to `_cpuid_gcc` to satisfy liblzma\u2019s original resolver contract.",
  "backdoor_init": "Converts the IFUNC entry context into a GOT patch: it initialises the GOT bookkeeping, locates the cpuid GOT slot via\n`update_got_address`, swaps the resolver pointer to `backdoor_init_stage2`, calls the genuine cpuid to finish initialisation,\nand then restores the slot back to its original target so future calls run the attacker\u2019s resolver without tripping sanity\nchecks.",
  "backdoor_init_stage2": "Runs inside the hijacked cpuid resolver. It builds temporary `backdoor_shared_globals_t`, `backdoor_hooks_ctx_t`, and\n`backdoor_setup_params_t` objects, repeatedly calls `init_hooks_ctx()` until the shared globals are available, and then hands\nthe bundle to `backdoor_setup`. If setup succeeds it never returns (the hooks stay installed); if setup fails it zeroes the GOT\ncontext and falls back to issuing a real CPUID so liblzma\u2019s resolver still fulfils glibc\u2019s contract.",
  "backdoor_setup": "The loader\u2019s main workhorse. It snapshots the caller\u2019s GOT/stack, builds a local `backdoor_data_t` describing all observed\nmodules, resolves sshd/libcrypto/liblzma/libc/ld.so via `process_shared_libraries`, initialises the shared globals, and pulls in\nthe `backdoor_hooks_data_t` blob sitting inside liblzma. With those pieces it refreshes the string-reference catalogue,\nconfigures the global context (payload buffers, sshd/log contexts, import tables), runs the sensitive-data + sshd-metadata\ndiscovery routines, and finally rewires ld.so\u2019s audit tables so `backdoor_symbind64` is invoked for every sshd\u2192libcrypto PLT\ncall. On success it copies the updated hook table back into liblzma and leaves the cpuid GOT slot ready to resume execution.",
  "bignum_serialize": "Normalises a BIGNUM into the [len||value] format used by the fingerprinting code. It caps inputs at 0x4000 bits, emits a 4-byte\nbig-endian length, copies the magnitude, and prepends a zero byte (or memmoves the data) whenever the top bit would otherwise\nmake the number negative so SHA-256 sees a canonical stream.",
  "c_memmove": "Private implementation of `memmove` so the object never has to import libc for something this trivial. It detects backwards\noverlap (`src < dest < src+cnt`) and copies from the end towards the beginning in that case; every other scenario devolves into\na forward copy loop. Either way the original `dest` pointer is returned so callers can chain copies just like they would with\nthe libc version.",
  "c_strlen": "Tiny strlen implementation that stage two uses before libc is trustworthy. It simply walks the buffer one byte at a time and\nreturns the length as a signed size, allowing other helpers to sanity-check argv/envp strings without resolving libc symbols.",
  "c_strnlen": "Bounded strlen variant used when scanning attacker-controlled buffers. It stops as soon as it sees a NUL or reaches `max_len`,\nreturning the limit unchanged if the string is unterminated so callers can treat that as an error.",
  "chacha_decrypt": "Checks the caller supplied pointers/lengths, verifies that the EVP entries in imported_funcs are non-null\n(contains_null_pointers), allocates an EVP_CIPHER_CTX, and runs EVP_chacha20 through Init/Update/Final. The helper enforces that\nthe final output length never exceeds the input, frees the context on every path, and reports TRUE only when all EVP calls\nsucceed.",
  "check_argument": "Walks a dash-prefixed argv entry two bytes at a time, mirroring each character so it can flag both upper- and lower-case\nvariants of '-d', '-D', '-E', '-Q', or any option that includes '=' or '/'. It returns the offending pointer so\n`process_is_sshd` can treat those switches as a hard stop and avoid touching sshd instances launched in debug or non-daemon\nmodes.",
  "check_backdoor_state": "Guards the payload assembly state machine. States 1\u20132 require a populated `sshd_payload_ctx` and a minimum payload length\n(>=0xae) plus a sane body_length pulled from the decrypted header; state 3 tolerates either 3 or 4; and state 0 expects the\nstaging buffer to be empty. Any inconsistency zeros the state and sets it to 0xffffffff so the hooks know to discard buffered\ndata.",
  "contains_null_pointers": "Linear check used before invoking crypto helpers. Given an array of pointers and a count, it reports 1 as soon as it encounters\na NULL slot, letting callers bail out if any required import failed to resolve.",
  "count_bits": "Wegner-style popcount loop over a 64-bit mask. The trie walker and the secret-data helpers use it to turn bitmap nodes into\nchild indexes without storing per-node counts.",
  "count_pointers": "Uses `malloc_usable_size()` to measure a pointer array and counts consecutive non-NULL entries until it hits either a NULL or\nthe allocation boundary. Sensitive-data heuristics call it when walking sshd tables whose length isn\u2019t stored explicitly.",
  "decrypt_payload_message": "Decrypts a ChaCha-wrapped `key_payload_t` chunk, copies the plaintext body into the global staging buffer when the advertised\nlength fits, and bumps `ctx->current_data_size`. The body is decrypted twice\u2014the second pass keeps the keystream in sync with\nsshd's original consumer\u2014 so later packets can continue appending without tearing, and any failure forces the payload state back\nto 0xffffffff.",
  "dsa_key_hash": "Pulls the p/q/g parameters and public key (y) out of the DSA handle via DSA_get0_pqg/DSA_get0_pub_key, serialises each with\nbignum_serialize into a 0x628-byte scratch buffer, and hashes the concatenation with sha256. Any missing pointer, oversized\nBIGNUM, or serialization failure aborts immediately so only genuine DSA host keys feed the fingerprint.",
  "elf_contains_vaddr": "Thin wrapper around `elf_contains_vaddr_impl` that keeps the public API surface simple. Every range-checker in the loader\nfunnels through it so the flag handling, recursion guard, and alignment fixes stay centralized, making it easy to detect when a\npointer falls outside the parsed ELF image.",
  "elf_contains_vaddr_impl": "Validates that `[vaddr, vaddr + size)` is entirely covered by one or more PT_LOAD segments whose `p_flags` mask includes the\nrequested bits. The helper page-aligns both ends of the interval, walks every loadable program header, and recurses when the\nrange straddles multiple segments so partial overlaps are rechecked piecemeal.\n\nIt refuses to run more than 0x3ea iterations (preventing runaway recursion), insists that the candidate addresses live inside\nthe mapped ELF image, and short-circuits to TRUE when `size` is zero. Callers pass `p_flags` values such as PF_X or PF_W to\ndifferentiate text, data, and RELRO spans.",
  "elf_contains_vaddr_relro": "Combines `elf_contains_vaddr` with the GNU_RELRO metadata harvested during `elf_parse`. The range must sit inside a read-only\nPT_LOAD (PF_R), and the module must have advertised a RELRO segment; if so the helper also verifies that `[vaddr, vaddr+size)`\nfalls within the page-aligned RELRO window cached in `elf_info_t`. Anything outside that protected span returns FALSE, which\nprevents the loader from treating writable data as RELRO by mistake.",
  "elf_find_function_pointer": "Takes a string-reference catalogue entry, locates the associated RELRO slot, and checks CET landing requirements before\nreturning the pointer. The loader relies on it to identify sshd callback tables\u2014such as monitor handlers\u2014that it will later\noverwrite with backdoor functions.",
  "elf_find_rela_reloc": "Searches the RELA relocation array for an entry tied to a given code pointer. When `encoded_string_id` is non-zero it is treated\nas an absolute address inside the module: the helper subtracts `elfbase` to match against `r_addend` and, on success, returns\nthe relocated slot at `r_offset`. When the argument is zero the caller instead wants the raw addend pointer, so the helper\nimmediately returns `elfbase + r_addend`.\n\nA pair of optional range bounds and a resumption index can be supplied in the additional SysV argument registers; if present\nthey force the returned address to fall inside `[low, high]` and let the caller continue scanning from the previous index.\nFailing to find a match (or discovering that the module never exposed RELA relocations) yields NULL and, if a cursor pointer was\nprovided, stores the position it stopped at.",
  "elf_find_relr_reloc": "Performs the same search as `elf_find_rela_reloc` but against the packed RELR format. It replays the RELR decoding algorithm\n(literal entry vs bitmap entry), sanity-checks each decoded pointer with `elf_contains_vaddr`, compares the pointed-to value\nagainst the requested target address, and optionally enforces a lower/upper bound plus an iteration cursor via the extra\nargument registers. Returning NULL means there were no RELR records, the address never appeared in the run, or one of the\ndecoded pointers failed validation.",
  "elf_find_string": "Iterates through the cached `.rodata` window, calling `get_string_id` on each byte offset until it encounters a recognizable\nencoded string. If `*stringId_inOut` is zero the first discovered string wins and its id is written back; otherwise the search\ncontinues until an exact id match is found. The optional `rodata_start_ptr` lets callers resume from a previous location or\nconstrain the search to a suffix of the segment.",
  "elf_find_string_reference": "Finds the first instruction that references a specific string literal between the supplied code bounds. The loader uses this\npinpoint search to anchor subsequent pattern matching when triangulating hook targets from log messages and status strings.",
  "elf_find_string_references": "Indexes interesting .rodata strings and the instructions that reference them, recording surrounding function bounds for later\nlookups. Many downstream heuristics consume this table to locate sshd routines and global pointers tied to sensitive behaviour.",
  "elf_get_code_segment": "Finds and caches the first executable PT_LOAD segment. The routine walks the program headers until it sees a segment with PF_X\nset, computes the runtime address by subtracting the ELF's minimum virtual address from `p_vaddr`, page-aligns both ends, stores\nthe start/size inside `elf_info_t`, and returns the aligned base while writing the computed size through `pSize`. Subsequent\ncalls use the cached values to avoid rescanning the headers.",
  "elf_get_data_segment": "Walks every PT_LOAD segment looking for the last read/write mapping (PF_W|PF_R). Once found it caches three pieces of\ninformation: the base of the mapped data (`data_segment_start`), the amount of padding between the end of the file-backed bytes\nand the next page boundary (`data_segment_alignment`), and the total size of the aligned segment. Callers either request the\ntrue segment span (`get_alignment == FALSE`) or the padding region (when TRUE), which is where the implant later tucks the\n`backdoor_hooks_data_t` structure.",
  "elf_get_got_symbol": "Identical pattern but aimed at the main RELA table: it requires `flags & 2` (meaning RELA records were found) and then calls\n`elf_get_reloc_symbol` with relocation type 6 (R_X86_64_GLOB_DAT). Successful lookups hand back the writable GOT slot for the\nsymbol so the loader can redirect it; failure means the symbol was not imported through a GOT relocation.",
  "elf_get_plt_symbol": "Looks up the PLT thunk for a given symbol by delegating to `elf_get_reloc_symbol` with relocation type 7 (R_X86_64_JUMP_SLOT).\nIt first makes sure the module actually advertised a PLT relocation table (flag bit 1) and caches its size, then returns the\nGOT/PLT entry that will be overwritten during hook installation. NULL means either the relocation table was absent or the\nrequested symbol never appeared there.",
  "elf_get_reloc_symbol": "Generic helper that scans an arbitrary relocation array for undefined symbols of a specific relocation type (e.g., GOT vs PLT)\nand a specific encoded name. It iterates through `num_relocs`, ensures the relocation type matches `reloc_type`, confirms the\nassociated symbol is really an import (`st_shndx == 0`), and then resolves the symbol name via `get_string_id` before comparing\nit to `encoded_string_id`. When it finds a match it returns the relocated address (`elfbase + r_offset`) so the caller can patch\nGOT/PLT entries in place.",
  "elf_get_rodata_segment": "Locates the first read-only PT_LOAD segment that lives entirely after the executable code. It first asks `elf_get_code_segment`\nfor the text range so it can ignore overlapping pages, then scans for PF_R-only segments, page-aligns their bounds, and picks\nthe lowest segment whose start is beyond the end of `.text`. The result is cached in `elf_info_t` and handed to callers\nalongside its size so later routines (string searches, RELRO probes) can reuse the computed window.",
  "elf_parse": "Initialises an `elf_info_t` from an in-memory ELF header: zeroes every field, records the lowest PT_LOAD virtual address,\nlocates the PT_DYNAMIC segment, and caches pointers to the strtab, symtab, relocation tables (PLT, RELA, RELR), GNU hash\nbuckets, version records, and GNU_RELRO metadata. Each pointer retrieved from the dynamic table is validated with\n`elf_contains_vaddr` so forged headers are rejected.\n\nIt also enforces invariants such as 'only one PT_GNU_RELRO segment', derives the number of dynamic entries, and flips feature\nbits (`flags`) so later helpers know whether RELR, versym, or gnurelro data is present. Failure to locate the dynamic segment,\nfind the required headers, or keep derived pointers inside mapped memory causes the parse to abort with FALSE.",
  "elf_symbol_get": "Symbol resolver that trusts the GNU hash table the loader extracted earlier. After setting a telemetry bit it walks each hash\nbucket, validates the bucket and chain addresses, and replays the classic GNU hash lookup to pull `Elf64_Sym` entries out of\n`.dynsym`. When a candidate symbol has a non-zero value and section index, the helper hashes the associated string with\n`get_string_id` and compares it against the requested encoded id.\n\nIf a symbol version is supplied it additionally consults `.gnu.version`/`.gnu.version_d`: the version index is read from\n`versym`, then matched against the verifier definitions by walking the linked `verdef` list and comparing the underlying string\nid. Returning NULL means either the target symbol is missing, the module did not advertise GNU hash+version tables, or the\nstring/relocation pointers failed validation.",
  "elf_symbol_get_addr": "Convenience layer on top of `elf_symbol_get`: look up the symbol, make sure it is defined (both `st_value` and `st_shndx` are\nnon-zero), and then turn the symbol value into a process address by adding it to `elf_info->elfbase`. Returning NULL indicates\neither the symbol does not exist or it represents an import/resolver stub that lacks a concrete address.",
  "extract_payload_message": "Scans an sshbuf blob for either 'ssh-rsa-cert-v01@openssh.com' or 'rsa-sha2-256', walks the surrounding length fields (network\nbyte order), and ensures the modulus chunk fits within the caller-provided buffer. When it finds a match it rewrites sshbuf->d\nto point at the modulus payload and returns its size so the command decoder knows how many bytes to decrypt.",
  "fake_lzma_alloc": "Companion to `fake_lzma_free` that turns the liblzma allocation API into a symbol resolver. The `opaque` parameter is treated as\nan `elf_info_t *`, the requested `size` is reinterpreted as an `EncodedStringId`, and it simply returns whatever\n`elf_symbol_get_addr()` produces. The `nmemb` argument is ignored because the helper is never asked to allocate real memory\u2014it\nonly masquerades as an allocator long enough to bootstrap symbol lookups inside ld.so.",
  "fake_lzma_free": "No-op placeholder that exists solely to satisfy the liblzma allocator interface the implant exposes. The loader wires this stub\ninto `lzma_allocator.free` until it can swap in the genuine host callbacks, so any invocation is guaranteed to do nothing other\nthan prove that the fake allocator is still active.\n\nHaving an inert body keeps the import surface small while still exporting a correctly typed symbol, and it gives the runtime a\nreliable indicator that a caller incorrectly tried to free memory through the bootstrap allocator.",
  "fd_read": "Wraps libc\u2019s read with retry logic. It refuses to run without both `read` and `__errno_location`, loops on EINTR, and aborts\nwith -1 when the helper sees EOF before the requested byte count. Successful reads consume the entire length so callers can\ntreat any non-zero return as \"buffer filled\".",
  "fd_write": "Mirror of `fd_read`: it requires valid write/errno pointers, retries on EINTR, and treats short writes as fatal so callers\neither send the entire buffer or receive -1. It is the plumbing used whenever the implant forges monitor messages.",
  "find_add_instruction_with_mem_operand": "Locates ADD instructions that update memory at a given address, capturing the scale of the increment. The scoring logic uses it\nto observe how sshd mutates counters so the implant can tag sensitive buffers.",
  "find_addr_referenced_in_mov_instruction": "Scans a referenced function for MOV instructions that materialise an address inside the supplied data window. The backdoor uses\nit to recover struct-field pointers (for example the monitor sockets) so it can redirect them to its own handlers.",
  "find_call_instruction": "Disassembles forward until it encounters a CALL opcode and reports both the instruction and target. The hook finder uses it to\nlocate indirect dispatcher sites in sshd so the injected shims can be spliced in safely.",
  "find_dl_audit_offsets": "Coordinates the entire ld.so reconnaissance pass. It resolves the necessary EC/EVP helpers via the fake allocator, copies\n`_dl_audit_symbind_alt`\u2019s address/size out of ld.so, and uses `find_link_map_l_name` to compute the displacement between the\ncached and live link_map entries. With that offset it invokes `find_dl_naudit` to capture the `_dl_naudit`/`_dl_audit` pointers\nand `find_link_map_l_audit_any_plt` to learn where the audit bit lives. Finally it copies libcrypto\u2019s basename into\n`hooks->ldso_ctx` so the forged `l_name` string looks correct. Any failure frees the temporary imports and aborts the audit-hook\ninstall path.",
  "find_dl_naudit": "Harvests the `_dl_naudit` counter and `_dl_audit` pointer from ld.so so the loader can toggle audit modules. After resolving\n`DSA_get0_pqg`, `DSA_get0_pub_key`, and `EVP_MD_CTX_free` via the fake allocator, it finds `rtld_global_ro`, searches for the\n`GLRO(dl_naudit)` string reference, and decodes the MOV that loads that slot. The same memory address is then matched inside\n`_dl_audit_symbind_alt`; if the MOV/TEST pair is found and the slot is still zero, the function records the `_dl_naudit` and\n`_dl_audit` pointers in `hooks->ldso_ctx`. Any deviation or pre-existing audit module aborts the attempt.",
  "find_function": "Combines the prologue scan with a forward sweep to determine both the start and end addresses of a function. Backdoor\ninitialization relies on it when it needs exact bounds for copying original bytes or scheduling follow-up pattern searches.",
  "find_function_prologue": "Sweeps backward from a code pointer looking for a plausible function prologue based on decoded instruction patterns. The runtime\nloader uses it to recover entry points in stripped sshd/libc images before installing hooks.",
  "find_instruction_with_mem_operand": "Convenience wrapper that searches for MOV/LEA forms touching a specific address and reports the displacement. It feeds higher-\nlevel routines that locate struct fields for the backdoor's runtime patch table.",
  "find_instruction_with_mem_operand_ex": "Performs a generic sweep for any instruction that touches memory, applying a caller-supplied predicate to filter the operands.\nThe loader routes specialised searches through it when reconstructing complex data flows in sshd.",
  "find_lea_instruction": "Finds the next LEA instruction in the stream and returns operand details. The backdoor uses this to recover base-plus-offset\ncalculations that point at data structures it later siphons.",
  "find_lea_instruction_with_mem_operand": "Restricts the LEA search to instructions that materialize a specific memory address, including displacement checks. It is\ninvoked when the implant needs to confirm the exact offset of sshd globals before patching them.",
  "find_link_map_l_audit_any_plt": "Primes an `instruction_search_ctx_t` before invoking the bitmask helper. It sweeps `_dl_audit_symbind_alt` for the LEA that\nmaterialises `link_map::l_name` using the caller-provided displacement, records which registers capture the pointer versus the\nmask, initialises the register filters/output buffers, and then calls `find_link_map_l_audit_any_plt_bitmask`. Success means\nboth the byte offset and AND mask are now stored in `hooks->ldso_ctx`; failure either means the pattern never appeared or the\nbit was already non-zero.",
  "find_link_map_l_audit_any_plt_bitmask": "Takes the displacement from `find_link_map_l_name` and hunts for the byte and mask that back ld.so\u2019s `link_map::l_audit_any_plt`\nflag. It temporarily resolves `EVP_DecryptInit_ex` and libc\u2019s `getuid`, decodes `_dl_audit_symbind_alt` with `x86_dasm`, and\ntracks which register holds the computed pointer. Once it sees the MOV-from-link_map followed by a TEST/BT it validates that the\nmask is a single set bit, records the absolute address in `hooks->ldso_ctx.sshd_link_map_l_audit_any_plt_addr`, stores the byte-\nwide mask, and insists the bit is still clear; otherwise the helper sets the search context\u2019s `result` flag and bails out.",
  "find_link_map_l_name": "Locates the live `link_map::l_name` byte inside ld.so and gathers the libc/libcrypto imports needed later in the run. It\npiggybacks on the fake `lzma_alloc` resolver to look up `exit`, `setlogmask`, `setresgid`, `setresuid`, `system`, `shutdown`,\nand `BN_num_bits`, then walks the cached liblzma link_map snapshot inside the binary until it finds the entry whose RELRO tuple\nmatches the running liblzma image. The resulting displacement becomes both `*libname_offset` and the pointer used to index\n`hooks->ldso_ctx.libcrypto_l_name`, and the helper double-checks that `_dl_audit_symbind_alt` references the same offset so\nlater code can safely rewrite the `l_name` field when posing as an audit module.",
  "find_mov_instruction": "Searches for MOV instructions with configurable load/store semantics and hands back the matched operands. It underpins many of\nthe signature searches the implant runs while deriving addresses for secret data or resolver trampolines.",
  "find_mov_lea_instruction": "Iterates through MOV and LEA instructions that move pointers into registers, honouring load/store direction flags. The loader\nuses it to chase GOT writes and frame setups when it has to recover sensitive pointers for the backdoor context.",
  "find_reg2reg_instruction": "Searches a code range for register-to-register moves while enforcing CET-safe constraints. The implant uses it when it needs to\nfollow pointer copies without touching memory operands during its pattern hunts.",
  "find_string_reference": "Scans for instructions that reference a given string literal via RIP-relative addressing and records the instruction span.\nSecret-data hunters use it to line up code blocks that print or parse target strings so hooks can score them.",
  "get_elf_functions_address": "Same pattern for the `elf_functions_t` dispatch table: start from the relocation-safe sentinel (`elf_functions_offset` lives\nnear `fake_lzma_allocator_offset` in `.data`) and advance 12 struct slots to arrive at the live table. The convoluted pointer\nmath lets the object carry offsets instead of absolute addresses, which keeps the relocation surface tiny while still giving the\nloader a stable way to reach its helper vtable.",
  "get_lzma_allocator": "Returns the `lzma_allocator` sub-structure embedded inside the fake allocator blob. Callers use it when they need to hand\nliblzma-style callbacks to another routine (e.g., passing an allocator into a liblzma API) while still pointing `opaque` at the\nimplant's `elf_info_t`.",
  "get_lzma_allocator_address": "Manual pointer arithmetic that recovers the runtime address of the fake `fake_lzma_allocator_t` blob without requiring\nrelocatable absolute addresses. The compiler emits a sentinel (`fake_lzma_allocator`) followed by padding, so this helper starts\nat that symbol and steps through the struct 12 times, effectively adding the baked-in 0x160-byte offset that lands on the real\nallocator instance the loader populated at build time.",
  "get_string_id": "Maps runtime strings to EncodedStringId identifiers without shipping plaintext literals. Each call logs itself via\nsecret_data_append_from_address, clamps the scan to 0x2c bytes, and walks two packed bitmaps (_Lcrc64_clmul_1+0x760 and the\nstring_action_data trie) while repeatedly calling count_bits to compute the next child index. It returns the encoded ID when it\nreaches a terminal node or 0 when the bytes miss the trie, and is used to find SSH banner strings during sshd heuristics.",
  "get_tls_get_addr_random_symbol_got_offset": "Seeds `ctx->got_ctx.got_ptr` and `ctx->got_ctx.got_offset` with the canned values associated with the fake `__tls_get_addr`\nsymbol. The loader uses those numbers as the starting point for `update_got_address`, which refines them into the concrete GOT\nentry address.",
  "hook_EVP_PKEY_set1_RSA": "Tap point for EVP_PKEY_set1_RSA so the backdoor sees every RSA handle even when the decrypt hook never fires. It simply calls\nrun_backdoor_commands on the key and then invokes the preserved OpenSSL routine.",
  "hook_RSA_get0_key": "Mirrors the same idea for RSA_get0_key: every consumer that asks OpenSSL for the modulus/exponent triggers run_backdoor_commands\nfirst, letting the implant inspect/track the RSA handle before delegating to the original function.",
  "hook_RSA_public_decrypt": "Replacement for RSA_public_decrypt: it ensures the PLT pointer is resolved, hands the RSA key to run_backdoor_commands (passing\na do_orig flag by reference), and either returns the backdoor\u2019s result or forwards the call to the genuine OpenSSL symbol\ndepending on whether the dispatcher consumed the ciphertext.",
  "init_elf_entry_ctx": "Seeds an `elf_entry_ctx_t` prior to running the IFUNC resolvers. It records the address of `cpuid_random_symbol`, captures the\ncaller's return address from the saved frame (slot 3), recomputes the GOT offset via `update_got_offset`, primes the cpuid GOT\nindex with `update_cpuid_got_index`, and clears the cached `got_ptr` so the resolver will refill it. The context is later\nconsumed by the GOT patching code that splices the malicious cpuid stub into sshd.",
  "init_hooks_ctx": "Primes a transient `backdoor_hooks_ctx_t` before stage two patches the GOT. It always points `hooks_data_addr` at the\n`hooks_data` blob baked into liblzma, zeros the scratch flags, and, when `ctx->shared` is still NULL, drops in the static hook\nentry points (`backdoor_symbind64`, the RSA shims, and the mm_* monitor hooks) before returning 0x65 so the caller can retry\nafter the shared globals are published. Once the shared block exists it simply returns 0, signalling that the structure now\ninherits every pointer from the shared globals.",
  "init_imported_funcs": "Sanity-checks the OpenSSL import table before the hooks are allowed to run. It requires `resolved_imports_count` to equal 0x1d\nand then inspects the `RSA_public_decrypt`, `EVP_PKEY_set1_RSA`, and `RSA_get0_key` PLT shims. If at least one of them is\nresolved it returns TRUE so later code can jump through the host's libcrypto. When all three slots are still NULL it plants\n`backdoor_init_stage2` / `init_shared_globals` in the RSA entries as crash-safe fallbacks and returns FALSE so stage two keeps\nwaiting until the imports are ready.",
  "init_ldso_ctx": "Restores every ld.so flag the implant may have touched: it writes the saved auditstate bindflags back to libcrypto/sshd, unsets\nthe copied `l_name` byte, clears the `l_audit_any_plt` bit with the mask recovered earlier, and zeros `_dl_naudit`/`_dl_audit`\nso the dynamic linker no longer believes an audit module is registered. Stage two calls it on failure paths so sshd resumes with\nthe original ld.so state.",
  "init_shared_globals": "Seeds the shared global block with the mm/EVP hook entry points and a pointer to the lone `global_ctx` instance. Every hook\nconsults this block at runtime, so the function simply wires the exported function pointers into the struct and returns success\nonce the pointer checks pass.",
  "is_endbr64_instruction": "Checks whether the bytes at the current cursor encode an ENDBR64 landing pad, including the CET prefix variations. The pattern\nscanners call it so the backdoor can safely step past CET trampolines while carving prologues to patch.",
  "is_gnu_relro": "Obfuscated equality test for PT_GNU_RELRO. Instead of comparing `p_type` directly against `0x6474e552`, the code adds the caller\nsupplied `addend` (always `0xa0000000`) and checks for the magic constant, which makes the instruction stream look less like a\nstraightforward RELRO probe in the object file.",
  "is_range_mapped": "Userland page-probe that avoids importing `mincore(2)`. The helper aligns the requested address downward, then walks one page at\na time toward `addr + length`, invoking the host's `pselect` with NULL fd sets and the page pointer passed in as the signal mask\nargument. If `pselect` faults with EFAULT the page is unmapped, otherwise the loop continues until every page succeeds. The\nroutine relies on `ctx->libc_imports` to surface both `pselect` and `__errno_location`, and it refuses to touch addresses below\n0x01000000 to avoid probing NULL or vsyscall.",
  "j_tls_get_addr": "Jumps straight into the real `__tls_get_addr` resolver. The backdoor keeps both this wrapper and the trapping stub exported so\nit can redirect GOT entries during setup: hooks call `j_tls_get_addr` when they want the legit resolver, while the relocation\nconstants point at the trapping version until the loader patches things up.",
  "lzma_alloc": "Counterpart to `lzma_free` that also traps. Once the loader installs the fake allocator the GOT entry is overwritten with\n`fake_lzma_alloc`; if execution ever reaches this stub it means the relocation failed and the safest option is to abort\nimmediately.",
  "lzma_check_init": "Intentional trap stub for liblzma's `lzma_check_init()`. Until the loader patches this export to the real liblzma routine it\nsimply calls `halt_baddata()`, guaranteeing that any accidental execution stops immediately and signalling that someone tried to\nrun the object outside the curated runtime.",
  "lzma_free": "Placeholder export for `lzma_free()` that funnels straight into `halt_baddata()`. The backdoor never expects this stub to run\nbecause it always routes work through the fake allocator, so entering it implies the GOT was not repointed and the process halts\nrather than corrupting memory.",
  "main_elf_parse": "Given a `main_elf_t` that already points at ld.so's ELF header, this routine parses the interpreter, looks up\n`__libc_stack_end`, and then calls `process_is_sshd` to verify that the captured runtime really belongs to sshd. If the checks\npass it stores the resolved `__libc_stack_end` pointer back through `main_elf->__libc_stack_end`, giving later stages an easy\nway to reach sshd's argument/environment block.",
  "mm_answer_authpassword_hook": "Responds to MONITOR_REQ_AUTHPASSWORD by either replaying the canned buffer stored in the global payload context or synthesising\na minimal success packet on the fly. The hook emits the reply through fd_write(), mirrors sshd's bookkeeping by updating the\nmonitor context at lVar1+0xa0, and returns 1 so the monitor thread believes password authentication succeeded without ever\nconsulting sshd.",
  "mm_answer_keyallowed_hook": "Drives the decrypted payload state machine: it extracts sshbuf chunks from the monitor message, pieces together and decrypts the\nstaged payload, validates signatures against the cached host_pubkeys, optionally runs privilege escalation (setresuid/setresgid\n+ system), and only then patches mm_answer_keyverify/mm_answer_authpassword before tail-calling the genuine\nmm_answer_keyallowed. On failure it resets the payload_state and, if instructed, exits sshd entirely.",
  "mm_answer_keyverify_hook": "Uses the cached monitor payload context to send the prebuilt MONITOR_ANS_KEYVERIFY reply directly to the requesting socket.\nAfter the write it restores the original mm_answer_keyverify function pointer so sshd's dispatcher advances as if the verifier\nsucceeded, and if the write fails it terminates sshd via the libc exit import to avoid leaving a half-patched state.",
  "mm_log_handler_hook": "Interposes on sshd's log handler, ignoring every message when logging is globally disabled or selectively rewriting the\n'Connection closed by ... (preauth)' lines when filtering mode is enabled. It rebuilds safe format strings on the stack, calls\nsshd_log() to emit the sanitised message, and leaves syslog alone unless the caller requested suppression via cmd flags.",
  "process_is_sshd": "Replays sshd's early argument parsing from the saved stack pointer: it verifies the argc/argv tuple is sane, checks argv[0]\nhashes to '/usr/sbin/sshd', walks every argument through `check_argument`, and then ensures envp pointers either live on the\nstack or inside the ELF .data segment. Any environment string that maps to a known identifier (get_string_id != 0) aborts the\nrun, which keeps the loader from running inside unexpected binaries or instrumentation harnesses.",
  "process_shared_libraries": "Wrapper around `process_shared_libraries_map` that first resolves `r_debug` out of ld.so, copies the caller-provided struct into\na local scratch copy, and feeds the scratch copy into the map-walker. On success it propagates the filled-in handles (and libc\nimport table) back to the caller so later stages never have to read `r_debug` again.",
  "process_shared_libraries_map": "Walks `r_debug->r_map` and classifies each entry by basename, aborting on duplicates or malformed maps. Only after locating sshd\n(the main binary), libcrypto, ld-linux, libsystemd, liblzma, and libc does it parse the ELF images: sshd\u2019s PLT is interrogated\nto recover `RSA_public_decrypt`, `EVP_PKEY_set1_RSA`, and `RSA_get0_key`, liblzma\u2019s RW data segment is recorded so the\n`backdoor_hooks_data_t` blob and `hooks_data_addr` can be cached, libcrypto/libc descriptors are primed for later walkers, and\nlibc\u2019s import table is filled via `resolve_libc_imports`. The result is a fully-populated `backdoor_shared_libraries_data_t` for\ndownstream stages.",
  "resolve_libc_imports": "Treats `link_map *libc` as another ELF image, runs `elf_parse` to populate `elf_info_t`, and then allocates trampolines for\n`read` and `__errno_location` via the fake allocator shim. Only when both imports succeed does it mark `libc_imports_t` as\nready, ensuring subsequent socket I/O helpers can operate without touching the real PLT.",
  "rsa_key_hash": "Grabs the exponent and modulus via RSA_get0_key, serialises the exponent first and the modulus second with bignum_serialize into\na ~4 KiB stack buffer, and runs sha256 over the exact number of bytes produced. Any missing component or overflow of the\n0x100a-byte scratch cancels the fingerprint.",
  "run_backdoor_commands": "Master dispatcher for the RSA hooks. It refuses to run unless the secret-data bitmap is complete, extracts the modulus and\nexponent via RSA_get0_key, and uses the modulus bytes as a transport for an encrypted payload header/body. The body is decrypted\nwith the ChaCha keys from secret_data_get_decrypted, every cached sshd host key is hashed (rsa_key_hash/dsa_key_hash/etc.) until\nthe embedded Ed448 signature verifies, and the resulting command toggles global_ctx state (sshd_offsets, syslog/PAM controls,\nsocket selection, payload streaming state). When a payload wants execution it populates a monitor_data_t and calls\nsshd_proxy_elevate; otherwise it patches sshd variables/logging in place. Any parse/signature failure sets\nctx->disable_backdoor, leaves *do_orig = TRUE, and the real OpenSSL routine proceeds untouched.",
  "secret_data_append_from_address": "Lets hooks fingerprint themselves without a static code pointer. If addr is NULL/1 it substitutes the caller\u2019s return address,\notherwise it uses the explicit address, and in either case it forwards both the call site and the resolved code pointer to\nsecret_data_append_singleton.",
  "secret_data_append_from_call_site": "Shortcut used directly inside hooks: it grabs the caller\u2019s return address (unaff_retaddr), runs secret_data_append_singleton\nwith it, and ORs the result with the supplied bypass flag so instrumentation sites can opt out when the attestation fails.",
  "secret_data_append_from_code": "Sweeps a code range and feeds instructions to secret_data_append_from_instruction. When start_from_call is TRUE it first finds\nthe next CALL via find_call_instruction, then loops up to shift_count times, each time calling find_reg2reg_instruction to\nlocate a qualifying instruction and shifting the supplied cursor. Returning FALSE means it could not find enough instructions in\nthe provided span.",
  "secret_data_append_from_instruction": "Sets the next bit inside global_ctx->secret_data based on a decoded instruction. The cursor enforces the 0x1C8-bit ceiling,\nskips certain opcodes (0x109, 0xBB, and entries in the precomputed 0x83\u20130xB1 mask), and otherwise locates the byte/bit inside\nthe secret_data array and ORs it in before advancing the cursor.",
  "secret_data_append_item": "Convenience wrapper used by the secret-data descriptor tables: when the supplied index is non-zero it simply calls\nsecret_data_append_singleton with the provided code pointer and cursor, otherwise it treats the entry as disabled and reports\nFALSE so the batch runner can bail out early.",
  "secret_data_append_items": "Batch driver for arrays of secret_data_item_t. It assigns sequential indexes to entries that have not been initialised yet,\ncalls the provided appender for each descriptor (passing the 1-based ordinal and the recorded code pointer), and stops at the\nfirst failure so callers know whether the entire batch completed.",
  "secret_data_append_singleton": "Guarantees each attestation slot runs at most once. It uses the per-operation byte array inside global_ctx->shift_operations to\nguard entry, resolves the function boundaries with find_function relative to the recorded sshd code limits, invokes\nsecret_data_append_from_code (starting after the call site if present), and increments global_ctx->num_shifted_bits by\nshift_count on success.",
  "secret_data_get_decrypted": "Unwraps the 57-byte global_ctx->secret_data blob with two ChaCha passes. A baked-in key/IV pair (the key_buf constants) decrypts\na 0x30-byte seed, that seed becomes the real ChaCha key, and a second decrypt peels the runtime secret into the caller buffer\nusing another static IV. All operations go through the resolved EVP entry points so no static crypto ships with the implant.",
  "sha256": "Thin wrapper around EVP_Digest/Evp_sha256: it rejects empty buffers, refuses to write unless mdBuf has at least 32 bytes of\nspace, looks up OpenSSL\u2019s SHA-256 implementation via the import table, and hashes the supplied payload in place.",
  "sshbuf_bignum_is_negative": "Treats an sshbuf as a serialized big integer. When the payload is between 0x20 and 0x40 bytes it scans forward until it finds a\nbyte with the sign bit set; encountering such a byte before hitting the end marks the buffer as \u201cnegative\u201d and therefore\nsuitable for modulus harvesting.",
  "sshbuf_extract": "Reads out an sshbuf\u2019s d pointer and size field using the dynamic layout encoded in global_ctx->sshd_offsets. Depending on\nwhether each qword index is negative it either uses the struct fields directly or walks to the encoded offset, confirms both the\nstruct and the pointed-to range are mapped with is_range_mapped, and hands the caller the live pointer/length pair.",
  "sshd_configure_log_hook": "Validates that the caller supplied a log context with writable handler slots, decides whether logging should be globally muted\nor merely filtered, and (when filtering) ensures all required format strings are present. It then captures the original\nhandler/context pair, optionally rewrites them if the pointers already point inside sshd, and drops in `mm_log_handler_hook` so\nforged monitor messages can suppress incriminating log lines.",
  "sshd_find_main": "Obtains sshd's code segment, decodes the entry stub, and looks for the instruction pair that loads the real `sshd_main` address\nright before the `__libc_start_main` thunk. When it sees a matching MOV/LEA that targets the GOT slot for libc's entry point it\nrecords the discovered `sshd_main`, resolves EVP_Digest/EVP_sha256, and caches the stub pointers inside `imported_funcs` so\nlater recon code can reuse them without reopening libcrypto.",
  "sshd_find_monitor_field_addr_in_function": "Sweeps a candidate sshd routine for MOV/LEA instructions that load a BSS slot into a register, confirms that the pointer flows\nunmodified into a nearby call to `mm_request_send`, and returns the underlying data-section address. The helper lets\n`sshd_find_monitor_struct` recover individual monitor fields (send/recv fds, sshbuf pointers, etc.) even when the surrounding\nfunction is stripped.",
  "sshd_find_monitor_struct": "Calls `sshd_find_monitor_field_addr_in_function` across ten monitor-related routines (accept/recv/send helpers, channel\nhandlers, etc.), tallies how many times each BSS address shows up, and picks the consensus pointer when at least five hits\nagree. The winner is stored in `ctx->struct_monitor_ptr_address` so later hooks can dereference monitor->m_sendfd/m_recvfd\ndirectly.",
  "sshd_find_sensitive_data": "Bootstraps the entire sensitive-data discovery pipeline: emits bookkeeping entries for the secret-data mirroring code, allocates\nlibcrypto stubs (EVP_DigestVerify*, EVP_CIPHER_CTX_new, EVP_chacha20), finds `sshd_main`/`uses_endbr64`, gathers code/data\nsegment bounds, and runs both the xcalloc and KRB5CCNAME heuristics. It scores whichever pointers were found, keeps the higher-\nconfidence candidate, and writes it into `ctx->sshd_sensitive_data` before returning success.",
  "sshd_get_client_socket": "Prefers using the recovered monitor struct: depending on DIR_READ/DIR_WRITE it fetches monitor->m_sendfd or m_recvfd, verifies\nthe fd by issuing a zero-length read that tolerates EINTR, and returns it on success. If the monitor pointer is missing or the\nfd is bad/EBADF it falls back to `sshd_get_usable_socket`'s fd scanner.",
  "sshd_get_sensitive_data_address_via_krb5ccname": "Starts at the string reference to 'KRB5CCNAME', disassembles forward until it sees the getenv result copied into memory, and\nonly accepts stores that land inside sshd's .data/.bss window with the expected -0x18 displacement pattern. That combination\nreliably identifies the sensitive_data struct that holds host key material after sshd propagates the Kerberos cache path.",
  "sshd_get_sensitive_data_address_via_xcalloc": "Locates the call site that matches the cached xcalloc reference, walks the following basic block looking for the MOV/LEA that\nparks the return value in .bss, and collects up to sixteen such stores. Whenever it sees three consecutive slots separated by 8\nbytes (pointer, pointer+8, pointer+0x10) it treats the lowest address as the sensitive_data candidate generated during sshd's\nearly zero-initialisation.",
  "sshd_get_sensitive_data_score": "Combines the three per-function heuristics with weighting: `demote_sensitive_data` and `main` scores get doubled and added\ntogether, then the `do_child` score is tacked on. Candidates must exceed the global threshold (>=8) before the pointer is\npublished to the rest of the implant.",
  "sshd_get_sensitive_data_score_in_demote_sensitive_data": "Disassembles the `demote_sensitive_data` helper referenced in the string table and returns three points if it ever references\nthe candidate pointer. That routine is highly specific to the real sensitive_data block, so even a single hit is treated as\nstrong evidence.",
  "sshd_get_sensitive_data_score_in_do_child": "Uses the string-reference catalogue to find `do_child`, then counts how often it dereferences the candidate pointer at offsets 0\nand +0x10. A hit on the base yields one point, and seeing multiple accesses to the +0x10 slot adds up to two more, producing a\nscore that reflects how tightly the child process manipulates the structure.",
  "sshd_get_sensitive_data_score_in_main": "Checks sshd's main() for memory operands that touch the candidate pointer at offsets 0, +8, and +0x10. The heuristic rewards\nroutines that touch the base and +0x10 entries while penalising ones that never reference +8, generating a small signed score\nthat later gets doubled in the aggregate calculation.",
  "sshd_get_sshbuf": "Finds the sshbuf inside sshd\u2019s monitor structure that now holds the forged modulus. It dereferences\nglobal_ctx->struct_monitor_ptr_address, uses the packed sshd_offsets to identify the m_pkex pointer and the sshbuf data/size\nfields, and validates any candidate via sshbuf_extract. When offsets are unknown it brute-forces the pkex table: two buffers\nmust decode to \u201cSSH-2.0\u201d/\u201cssh-2.0\u201d string IDs and the next buffer must look like a negative bignum (sshbuf_bignum_is_negative).\nOnly then does it return the mapped sshbuf->d pointer and length.",
  "sshd_get_usable_socket": "Linearly probes file descriptors 0\u201363, calling shutdown(fd, SHUT_RDWR) and treating errors like EINVAL/ENOTCONN as evidence that\nthe descriptor is alive but idle. Each qualified descriptor increments a counter, and when it matches `socket_index` the fd is\nreturned so the implant can recycle sshd's sockets without holding a monitor struct.",
  "sshd_log": "Wraps sshd's sshlogv() implementation by rebuilding a va_list on the stack, saving/restoring XMM registers when necessary, and\nthen tail-calling the resolved function pointer in the log context. Every monitor hook routes formatted log lines through here\nso it matches OpenSSH's logging ABI without needing libc wrappers.",
  "sshd_patch_variables": "Requires the mm_answer_authpassword hook to be resolved, then optionally forces PermitRootLogin to 'yes', disables PAM when\nrequested, and swaps the monitor authpassword function pointer to the implant's hook. If no explicit monitor_reqtype override is\nprovided it derives the current request ID from the original function pointer so replies continue matching sshd's state machine.",
  "sshd_proxy_elevate": "Implements the privileged side of the monitor command channel. Depending on the cmd_type and flags it may disable PAM, short-\ncircuit non interactive requests, or exit when instructed. For KEYALLOWED-style payloads it hunts for the staged ChaCha-wrapped\nblob on the stack, decrypts it with the recovered key, generates a signed MONITOR_REQ_KEYALLOWED packet using freshly built\nRSA/BIGNUM objects and the attacker-provided modulus/exponent, and writes the forged request over the selected monitor or\nfallback socket. It then pushes any extra sshbuf data when needed, drains the reply, and honours 'exit' or 'wait for response'\nsemantics encoded in the original command.",
  "update_cpuid_got_index": "Copies the relocation constants baked into `tls_get_addr_reloc_consts` into `ctx->got_ctx.cpuid_fn`. That value is the GOT index\nof the cpuid resolver inside liblzma, so later code can patch the correct slot without rescanning the PLT stub.",
  "update_got_address": "Disassembles liblzma's `__tls_get_addr` PLT stub, accounts for the short/long JMP encodings, and then computes the true GOT\nentry by applying the stub's 32-bit displacement. The resulting pointer is cached in `ctx->got_ctx.got_ptr` and later consumed\nwhen swapping the cpuid GOT slot over to the implant's resolver.",
  "update_got_offset": "Copies `_Llzma_block_buffer_decode_0` into `ctx->got_ctx.got_offset`, giving the loader a reproducible base when translating\nbetween the baked relocation constants and runtime addresses. It pairs with `update_got_address` during the cpuid GOT patch.",
  "validate_log_handler_pointers": "Given two candidate addresses for sshd\u2019s `log_handler`/`log_handler_ctx` globals, it replays the code sequence that writes them.\nThe helper enforces that the pointers are distinct and within 0x10 bytes of one another, walks the cached string-reference\nentries to find the LEA that materialises the handler struct, bounds the routine via `x86_dasm`/`find_function`, and then\nsearches for MOV [mem],reg instructions touching each address. Only when both stores appear inside that function does it accept\nthe pair as the genuine log-handler slots.",
  "verify_signature": "Computes the host-key digest that sits at sshkey_digest_offset inside the signed blob and then verifies the Ed448 command\nsignature. RSA and DSA keys delegate to rsa_key_hash/dsa_key_hash, ECDSA serialises the EC_POINT in uncompressed form with a\n32-bit length prefix, and Ed25519 prepends a 0x20000000 tag plus the raw 32-byte key. Once the digest is spliced into\nsigned_data the helper loads the attacker\u2019s Ed448 public key with EVP_PKEY_new_raw_public_key(0x440, \u2026) and invokes\nEVP_DigestVerify over the signed_data[0:tbslen) region; only a valid Ed448 signature lets the caller continue.",
  "x86_dasm": "Implements a minimal x86-64 decoder that walks a buffer while tracking instruction metadata. Every search helper in the loader\nuses it to reason about sshd and ld.so machine code without linking a full disassembler, giving the backdoor reliable patch\ncoordinates at runtime.",
  "xzre_globals": "Symbol that aliases the packed `backdoor_hooks_data_t` blob liblzma keeps in its RW data segment. The block combines the\nloader\u2019s working state (`ldso_ctx_t` with every `_dl_audit` pointer and GOT snapshot), the shared `global_context_t` (payload\nbuffers, sshd/log/secret-data state), the resolved OpenSSL/libc import tables, the sshd/log metadata, and the signed payload\nbytes themselves. Stage two writes into this object and the mm/EVP hooks read from it via the `hooks_data`/`global_ctx` pointers\npublished by `init_shared_globals`."
}

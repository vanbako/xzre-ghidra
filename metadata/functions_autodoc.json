{
  "_cpuid_gcc": "GCC-style CPUID shim that dispatches through the individual helper thunks for every supported leaf (basic, cache, topology,\nextended brand strings, etc.). Whatever leaf pointer it chooses has EAX/EBX/ECX/EDX copied into the provided outputs so callers\ndon’t need inline assembly.",
  "_get_cpuid_modified": "Wrapper around `_cpuid_gcc` that first invokes `backdoor_entry` with the high-bit leaf to make sure the loader ran, checks the\nreturned maximum leaf, and only executes the requested CPUID if the CPU claims to support it. This is the exported symbol glibc\nbinds, so the loader’s work is triggered before any sshd thread asks for cpuid data.",
  "backdoor_entry": "IFUNC resolver entry point. It increments a global invocation counter, calling `backdoor_init()` on the second pass so the\nloader can stage its hooks while glibc thinks it is still choosing a cpuid implementation. Regardless of setup, it finally\ndelegates to `_cpuid_gcc` to satisfy liblzma’s original resolver contract.",
  "backdoor_init": "Converts the IFUNC entry context into a GOT patch: it initialises the GOT bookkeeping, locates the cpuid GOT slot via\n`update_got_address`, swaps the resolver pointer to `backdoor_init_stage2`, calls the genuine cpuid to finish initialisation,\nand then restores the slot back to its original target so future calls run the attacker’s resolver without tripping sanity\nchecks.",
  "backdoor_init_stage2": "Runs inside the hijacked cpuid resolver. It builds temporary `backdoor_shared_globals_t`, `backdoor_hooks_ctx_t`, and\n`backdoor_setup_params_t` objects, repeatedly calls `init_hooks_ctx()` until the shared globals are available, and then hands\nthe bundle to `backdoor_setup`. If setup succeeds it never returns (the hooks stay installed); if setup fails it zeroes the GOT\ncontext and falls back to issuing a real CPUID so liblzma’s resolver still fulfils glibc’s contract.",
  "backdoor_setup": "The loader’s main workhorse. It snapshots the caller’s GOT/stack, builds a local `backdoor_data_t` describing all observed\nmodules, resolves sshd/libcrypto/liblzma/libc/ld.so via `process_shared_libraries`, initialises the shared globals, and pulls in\nthe `backdoor_hooks_data_t` blob sitting inside liblzma. With those pieces it refreshes the string-reference catalogue,\nconfigures the global context (payload buffers, sshd/log contexts, import tables), runs the sensitive-data + sshd-metadata\ndiscovery routines, and finally rewires ld.so’s audit tables so `backdoor_symbind64` is invoked for every sshd→libcrypto PLT\ncall. On success it copies the updated hook table back into liblzma and leaves the cpuid GOT slot ready to resume execution.",
  "bignum_serialize": "Normalises a BIGNUM into the [len||value] format used by the fingerprinting code. It caps inputs at 0x4000 bits, emits a 4-byte\nbig-endian length, copies the magnitude, and prepends a zero byte (or memmoves the data) whenever the top bit would otherwise\nmake the number negative so SHA-256 sees a canonical stream.",
  "c_memmove": {
    "plate": "Private implementation of `memmove` so the object never has to import libc for something this trivial. It detects backwards\noverlap (`src < dest < src+cnt`) and copies from the end towards the beginning in that case; every other scenario devolves into\na forward copy loop. Either way the original `dest` pointer is returned so callers can chain copies just like they would with\nthe libc version.",
    "inline": [
      {
        "match": "if ((src < dest) && (dest < src + cnt)) {",
        "comment": "Backward overlap means copy from the tail first so the source bytes are not clobbered mid-move."
      },
      {
        "match": "\\s*dest\\[forward_idx\\] = src\\[forward_idx\\];",
        "match_type": "regex",
        "comment": "Linear forward copy covers every other configuration where the ranges do not overlap."
      }
    ]
  },
  "c_strlen": "Tiny strlen implementation that stage two uses before libc is trustworthy. It simply walks the buffer one byte at a time and\nreturns the length as a signed size, allowing other helpers to sanity-check argv/envp strings without resolving libc symbols.",
  "c_strnlen": "Bounded strlen variant used when scanning attacker-controlled buffers. It stops as soon as it sees a NUL or reaches `max_len`,\nreturning the limit unchanged if the string is unterminated so callers can treat that as an error.",
  "chacha_decrypt": "Checks the caller supplied pointers/lengths, verifies that the EVP entries in imported_funcs are non-null\n(contains_null_pointers), allocates an EVP_CIPHER_CTX, and runs EVP_chacha20 through Init/Update/Final. The helper enforces that\nthe final output length never exceeds the input, frees the context on every path, and reports TRUE only when all EVP calls\nsucceed.",
  "check_argument": {
    "plate": "Slides a two-byte window across dash-prefixed argv entries and reports the first position whose bytes contain lowercase `d`. `process_is_sshd` treats that non-NULL pointer as proof sshd was launched with `-d`/`--debug`, so the loader stays away from debug-mode daemons. Control bytes, tabs, and `=` terminate the walk early and force a NULL return so only clean switches reach the matcher.",
    "inline": [
      {
        "match": "if (arg_first_char == '-') {",
        "comment": "Only inspect argv entries that began with `-`; everything else returns NULL immediately."
      },
      {
        "match": "window_chars = \\*\\(ushort \\*\\)arg_name;",
        "match_type": "regex",
        "comment": "Load two characters at a time so the loop can compare both bytes without calling strlen()."
      },
      {
        "match": "if ((following_char_word == 0x6400) || (current_char_word == 0x6400))",
        "comment": "Stop as soon as either byte in the window equals lowercase `d` and return that pointer."
      },
      {
        "match": "if ((((window_chars & 0xdf00) == 0) ||",
        "match_type": "wildcard",
        "comment": "Abort the scan when the pair contains control characters, TAB, or `=`—those inputs fall through to NULL."
      },
      {
        "match": "arg_name = (char *)((long)arg_name + 2);",
        "comment": "Advance by one `ushort` (two more characters) before the next comparison."
      }
    ]
  },
  "check_backdoor_state": {
    "plate": "Enforces the payload assembly state machine kept in `global_context_t`: states 1 and 2 require a populated\n`sshd_payload_ctx`, at least 0xae bytes buffered, and a sane body_length lifted from the decrypted header (including\nroom for the 0x60-byte trailer). State 0 insists that the staging buffer stays smaller than 0xae bytes, while state 3/4\naccept only those literal values. Any mismatch resets `payload_state` to 0xffffffff so the caller knows to discard\npartially buffered data.",
    "inline": [
      {
        "match": "if (((ctx->payload_ctx != (sshd_payload_ctx_t *)0x0) && (0xad < ctx->payload_bytes_buffered))",
        "match_type": "wildcard",
        "comment": "States 1 and 2 only pass when the decrypted header exists, >= 0xae bytes are buffered, and the advertised payload (plus the 0x60-byte trailer) fits inside `payload_buffer_size`."
      },
      {
        "match": "state_in_expected_range = ctx->payload_bytes_buffered < 0xae;",
        "comment": "State 0 is just a guardrail—once the staging buffer hits 0xae bytes the caller must graduate into state 1."
      },
      {
        "match": "state_in_expected_range = payload_state == 3;",
        "comment": "States 3 and 4 are literal sentinels; any other value immediately fails the check."
      },
      {
        "match": "ctx->payload_state = 0xffffffff;",
        "comment": "Any violation poisons the state machine so callers drop the partially buffered ciphertext and restart."
      }
    ]
  },

  "contains_null_pointers": "Linear check used before invoking crypto helpers. Given an array of pointers and a count, it reports 1 as soon as it encounters\na NULL slot, letting callers bail out if any required import failed to resolve.",
  "count_bits": "Wegner-style popcount loop over a 64-bit mask. The trie walker and the secret-data helpers use it to turn bitmap nodes into\nchild indexes without storing per-node counts.",
  "count_pointers": "Uses `malloc_usable_size()` to measure a pointer array and counts consecutive non-NULL entries until it hits either a NULL or\nthe allocation boundary. Sensitive-data heuristics call it when walking sshd tables whose length isn’t stored explicitly.",
  "decrypt_payload_message": {
    "plate": "Decrypts a ChaCha-wrapped `key_payload_t` chunk using the attacker-provided key material returned by\n`secret_data_get_decrypted`. If the header/body lengths are sane and there is enough space left in `ctx->payload_buffer`,\nit copies the plaintext body into the staging buffer, bumps `payload_bytes_buffered`, and then replays the decryption a\nsecond time so the ChaCha keystream stays aligned with sshd's original consumer. Any failure (bad lengths, short\ndecrypts, exhausted buffer) forces `payload_state` back to 0xffffffff so future packets start from a clean slate.",
    "inline": [
      {
        "match": "decrypt_ok = secret_data_get_decrypted((u8 *)&payload_key_seed,ctx);",
        "comment": "Recover the ChaCha key/IV pair from the encrypted secret blob before touching the ciphertext."
      },
      {
        "match": "decrypt_ok = chacha_decrypt((u8 *)body_length_cursor,inl,(u8 *)&payload_key_seed,(u8 *)&hdr,(u8 *)body_length_cursor,",
        "occurrence": 1,
        "comment": "First pass decrypts the header/trailer in place so the claimed body length can be validated."
      },
      {
        "match": "for (copy_idx = 0; body_len != copy_idx; copy_idx = copy_idx + 1) {",
        "comment": "Copy the plaintext body directly into `ctx->payload_buffer`, preserving the stream order."
      },
      {
        "match": "decrypt_ok = chacha_decrypt((u8 *)body_length_cursor,inl,(u8 *)&payload_key_seed,(u8 *)&hdr,(u8 *)body_length_cursor,",
        "occurrence": 2,
        "comment": "Re-run the decrypt so ChaCha’s keystream pointer stays aligned with sshd’s original consumer."
      },
      {
        "match": "ctx->payload_state = 0xffffffff;",
        "comment": "Any validation failure poisons the state machine so the caller restarts the stream from scratch."
      }
    ]
  },
  "dsa_key_hash": "Pulls the p/q/g parameters and public key (y) out of the DSA handle via DSA_get0_pqg/DSA_get0_pub_key, serialises each with\nbignum_serialize into a 0x628-byte scratch buffer, and hashes the concatenation with sha256. Any missing pointer, oversized\nBIGNUM, or serialization failure aborts immediately so only genuine DSA host keys feed the fingerprint.",
  "elf_contains_vaddr": {
    "plate": "Thin wrapper around `elf_contains_vaddr_impl` that keeps the public API surface simple. Every range-checker in the loader funnels through it so the flag handling, recursion guard, and alignment fixes stay centralized, making it easy to detect when a pointer falls outside the parsed ELF image.",
    "inline": [
      {
        "match": "range_ok = elf_contains_vaddr_impl(",
        "match_type": "wildcard",
        "comment": "Delegate the heavy lifting (alignment, recursion depth, and flag filtering) to the recursive helper."
      }
    ]
  },
  "elf_contains_vaddr_impl": {
    "plate": "Validates that `[vaddr, vaddr + size)` is entirely covered by one or more PT_LOAD segments whose `p_flags` mask includes the requested bits. The helper page-aligns both ends of the interval, walks every loadable program header, and recursively re-checks any prefix/suffix that straddles adjacent segments until the entire interval is proven resident.\n\nIt refuses to run more than 0x3ea iterations (preventing runaway recursion), insists that the candidate addresses live inside the mapped ELF image, and short-circuits to TRUE when `size` is zero. Callers pass `p_flags` values such as PF_X or PF_W to differentiate text, data, and RELRO spans.",
    "inline": [
      {
        "match": "segment_page_floor = (Elf64_Ehdr *)(segment_runtime_start & 0xfffffffffffff000);",
        "match_type": "wildcard",
        "comment": "Align each candidate PT_LOAD window to page boundaries so the comparison never straddles partial pages."
      },
      {
        "match": "if ((segment_page_ceil < range_limit) && (segment_page_floor > vaddr)) {",
        "match_type": "wildcard",
        "comment": "Range pierces both edges of this segment—split it into left/right halves and validate them recursively."
      },
      {
        "match": "else if (segment_page_ceil < range_limit) {",
        "match_type": "wildcard",
        "comment": "Otherwise advance `vaddr` past the current segment and continue checking the remaining bytes."
      }
    ]
  },
  "elf_contains_vaddr_relro": {
    "plate": "Extends `elf_contains_vaddr` with GNU_RELRO bounds checking. The helper first reuses the normal containment test (forcing PF_R) so `[vaddr, vaddr+size)` is known to live inside a readable PT_LOAD. When `p_flags` is non-zero and the ELF exported PT_GNU_RELRO metadata it converts the RELRO segment into runtime pointers, page-aligns the window, and verifies the caller's span is fully enclosed. Requests outside the RELRO range (or binaries that never exposed RELRO) return FALSE so later hooks never mis-tag writable memory.",
    "inline": [
      {
        "match": "range_is_protected = elf_contains_vaddr(elf_info,(void *)vaddr,size,2);",
        "match_type": "wildcard",
        "comment": "Leverage the generic helper to ensure the range already lives inside a readable PT_LOAD segment."
      },
      {
        "match": "if (((range_is_protected != FALSE) && (range_is_protected = TRUE, p_flags != 0)) && (elf_info->gnurelro_present != FALSE)) {",
        "match_type": "wildcard",
        "comment": "`p_flags` acts as a caller-supplied \"must be RELRO\" bit—only then do we enforce the PT_GNU_RELRO bounds."
      },
      {
        "match": "if ((relro_window_end <= vaddr) || (range_is_protected = FALSE, vaddr < relro_window_start)) {",
        "match_type": "wildcard",
        "comment": "Clamp to the page-aligned RELRO span; any byte outside `[relro_window_start, relro_window_end)` fails the containment test."
      }
    ]
  },
  "elf_find_function_pointer": {
    "plate": "Looks up the string-reference entry keyed by `xref_id`, copies its cached `[func_start, func_end)` range into the caller’s outputs, and then hunts for a relocation that targets that start address.\nIt prefers RELA records (matching `r_addend` against the function pointer) and falls back to RELR bitmaps when the addend table is missing; whichever relocation hits becomes the writable slot returned via `pOutFptrAddr`.\nSuccess requires that slot to sit inside GNU_RELRO and, when CET telemetry says sshd uses ENDBR, a final `is_endbr64_instruction` check ensures the callee still begins with ENDBR so we never patch a stale helper. Missing xrefs or relocations immediately return FALSE.",
    "inline": [
      {
        "match": "rela_match = elf_find_rela_reloc(",
        "match_type": "wildcard",
        "comment": "Prefer RELA so we match the explicit addend/GOT slot before bothering with the packed RELR table."
      },
      {
        "match": "relr_match = elf_find_relr_reloc(",
        "match_type": "wildcard",
        "comment": "RELR fallback covers PIE builds where the GOT slot only appears inside the bitmap run."
      },
      {
        "match": "slot_valid = elf_contains_vaddr_relro(",
        "match_type": "wildcard",
        "comment": "Only hand back slots inside GNU_RELRO; anything outside hardened memory is rejected."
      },
      {
        "match": "if (ctx->uses_endbr64 != FALSE) {",
        "match_type": "wildcard",
        "comment": "CET builds expect ENDBR at entry, so double-check the cached function really still starts with it."
      }
    ]
  },
  "elf_find_rela_reloc": {
    "plate": "Searches the RELA relocation array for an entry tied to a given code pointer. When `encoded_string_id` is non-zero it is treated\nas an absolute address inside the module: the helper subtracts `elfbase` to match against `r_addend` and, on success, returns\nthe relocated slot at `r_offset`. When the argument is zero the caller instead wants the raw addend pointer, so the helper\nimmediately returns `elfbase + r_addend`.\n\nCallers can optionally supply `[low, high]` bounds (third argument + RCX) and a resume cursor (R8). Those extra registers force the\nreturned address to fall inside a desired window and allow the next invocation to continue scanning without starting over.\nFailing to find a match (or discovering that the module never exposed RELA relocations) yields NULL and, if a cursor pointer was\nprovided, stores the position it stopped at.",
    "inline": [
      {
        "match": "if (((elf_info->feature_flags & 2) == 0) || (elf_info->rela_reloc_count == 0)) {",
        "match_type": "wildcard",
        "comment": "Bail out immediately when the module never published RELA entries."
      },
      {
        "match": "if ((int)rela_cursor->r_info == 8) {",
        "match_type": "wildcard",
        "comment": "Only R_X86_64_RELATIVE entries are interesting here; everything else is ignored."
      },
      {
        "match": "if ((slot_lower_bound <= rela_cursor) && (rela_cursor <= slot_upper_bound)) {",
        "match_type": "wildcard",
        "comment": "Honor the optional `[low, high]` window before handing the relocation slot back to the caller."
      }
    ]
  },
  "elf_find_relr_reloc": {
    "plate": "Performs the same search as `elf_find_rela_reloc` but against the packed RELR bitmap stream. The helper requires\nRELR metadata (feature bit 4), rebuilds literal entries vs. bitmap runs into candidate pointers, validates each\npointer with `elf_contains_vaddr`, and compares the dereferenced value against the requested absolute address.\nOptional `[slot_lower_bound, slot_upper_bound]` and `resume_index_ptr` parameters let callers clamp the acceptable\nslot range and resume the walk mid-stream. NULL means the module had no RELR entries, none targeted the requested\naddress, or the decoded pointer failed validation.\n",
    "inline": [
      {
        "match": "if ((elf_info->feature_flags & 4) != 0) {",
        "match_type": "wildcard",
        "comment": "RELR support is optional—bail immediately when the module never published bitmap metadata."
      },
      {
        "match": "if ((relr_entry & 1) == 0) {",
        "match_type": "wildcard",
        "comment": "Literal entries carry an absolute pointer; validate it and compare the stored addend once."
      },
      {
        "match": "while (relr_entry = relr_entry >> 1, relr_entry != 0) {",
        "match_type": "wildcard",
        "comment": "Bitmap entries expand into 63 consecutive slots—each set bit hands back another 8-byte pointer."
      },
      {
        "match": "if ((*(Elf64_Relr *)relr_slot_ptr == target_offset) &&",
        "match_type": "wildcard",
        "comment": "Only return matches that land inside the optional `[slot_lower_bound, slot_upper_bound]` window."
      }
    ]
  },
  "elf_find_string": {
    "plate": "Scans the cached `.rodata` window for encoded literals. After logging telemetry it asks `elf_get_rodata_segment` for the base/span (bailing if the segment is shorter than 0x2c bytes), optionally clamps the starting cursor to `rodata_start_ptr`, and then advances one byte at a time calling `get_string_id(cursor, rodata_end)`. If `*stringId_inOut` is zero the first non-zero id wins and is written back; otherwise the search continues until the requested id reappears.\n\nThe return value is the pointer where the literal begins, making it easy to resume subsequent scans or correlate the literal with code references.",
    "inline": [
      {
        "match": "telemetry_ok = secret_data_append_from_call_site(",
        "match_type": "wildcard",
        "comment": "Skip the expensive rodata walk entirely when the secret-data logger rejects the breadcrumb."
      },
      {
        "match": "if (rodata_start_ptr != (void *)0x0) {",
        "match_type": "wildcard",
        "comment": "Let callers resume from a previous offset or clamp the scan to a sub-range of `.rodata`."
      },
      {
        "match": "if (*stringId_inOut == 0) {",
        "match_type": "wildcard",
        "comment": "Treat a zero id as \"take the first literal we decode\"; otherwise demand an exact id match."
      }
    ]
  },
  "elf_find_string_reference": {
    "plate": "Single-slot helper for the string catalogue. It emits a secret-data breadcrumb, then repeatedly calls `elf_find_string` (resuming from the last rodata pointer) until the requested `encoded_string_id` is encountered.\nEach discovery is checked with `find_string_reference` inside `[code_start, code_end)`, and the first instruction that materialises the literal is returned so later passes can clamp function ranges around it. Telemetry failures or missing xrefs return NULL.",
    "inline": [
      {
        "match": "telemetry_ok = secret_data_append_from_call_site(",
        "match_type": "wildcard",
        "comment": "Abort the scan if we cannot log the breadcrumb—string hunts only run when the secret-data recorder is active."
      },
      {
        "match": "while (string_cursor = elf_find_string(",
        "match_type": "wildcard",
        "comment": "Walk `.rodata`, resuming from the previous pointer so every occurrence of the encoded literal is examined."
      },
      {
        "match": "xref_site = find_string_reference(",
        "match_type": "wildcard",
        "comment": "Return the first LEA/MOV inside the requested code window that materialises this literal."
      }
    ]
  },
  "elf_find_string_references": {
    "plate": "Populates the 27-entry `string_references_t` table that anchors every sshd heuristic.\nIt seeds each slot with its fixed `EncodedStringId`, scrapes `.rodata` with `elf_find_string`, and latches the first LEA that materialises every literal via `find_string_reference`.\nThe routine then decodes `.text` start-to-finish with `x86_dasm`, shrinking each `[func_start, func_end)` around CALLs, PLT/JMPs, and RIP-relative LEAs that hit the recorded xrefs, and finally folds in RELA/RELR relocation hits plus code-segment bounds so the resulting ranges are trustworthy and always executable.",
    "inline": [
      {
        "match": "(entry_cursor->xcalloc_zero_size).string_id = string_id_seed;",
        "match_type": "wildcard",
        "comment": "Pre-seed every slot with the encoded literal it should track before the heavy scans begin."
      },
      {
        "match": "xref_instruction = find_string_reference(",
        "match_type": "wildcard",
        "comment": "Record the first LEA/MOV that materialises each literal so the later range tightening has an anchor."
      },
      {
        "match": "insn_decoded = x86_dasm(",
        "match_type": "wildcard",
        "comment": "Walk the entire text range with the decoder, tightening function bounds whenever a CALL/JMP/LEA targets our xrefs."
      },
      {
        "match": "while (rela_slot = elf_find_rela_reloc(",
        "match_type": "wildcard",
        "comment": "Sweep the relocation tables too so GOT/PLT slots that touch the literal keep the enclosing range in view."
      },
      {
        "match": "if (text_segment_end <= scratch_ctx) {",
        "match_type": "wildcard",
        "comment": "Clamp any straggling ranges/xrefs back inside `.text` so later scans cannot wander past executable memory.",
        "occurrence": 1
      }
    ]
  },
  "elf_get_code_segment": {
    "plate": "Finds and caches the first executable PT_LOAD segment. The helper emits a telemetry breadcrumb, walks the program\nheaders until it sees PF_X set, converts `p_vaddr` into a runtime pointer (subtracting the ELF load base), page\naligns the `[start, end)` window, and records both the base and span in `elf_info_t`. Subsequent calls reuse the\ncached `text_segment_start/size` so the expensive scan only happens once.\n",
    "inline": [
      {
        "match": "telemetry_ok = secret_data_append_from_address(",
        "match_type": "wildcard",
        "comment": "Emit a secret-data breadcrumb before touching program headers so text discovery stays audited."
      },
      {
        "match": "code_segment_start = (void *)elf_info->text_segment_start;",
        "match_type": "wildcard",
        "comment": "Once the text range is cached, future callers reuse it instead of rescanning the headers."
      },
      {
        "match": "if ((phdr->p_type == 1) && ((phdr->p_flags & 1) != 0)) {",
        "match_type": "wildcard",
        "comment": "Pick the first executable PT_LOAD entry and align its `[start, end)` window to page boundaries."
      }
    ]
  },
  "elf_get_data_segment": {
    "plate": "Locates and caches the writable PT_LOAD that carries `.data` and `.bss`. Repeat callers reuse `data_segment_start/size` (or, when `get_alignment` is TRUE, the cached padding between the live data and the next page) so the expensive scan runs only once. On a cold call the helper walks every PT_LOAD with PF_W|PF_R, validates `p_memsz >= p_filesz`, converts the virtual range into a runtime pointer, page-aligns `[start,end)`, and retains the candidate whose aligned end sits highest in memory. The winner's file-backed end, zero-filled tail, and page-rounded padding are stored in `elf_info_t` and returned through `pSize` so later hooks can either reach `.data` or carve out the staging padding for `backdoor_hooks_data_t`.",
    "inline": [
      {
        "match": "if (cached_data_start != (void *)0x0) {",
        "match_type": "wildcard",
        "comment": "Fast path: reuse the cached `.data` pointer/span or, when asked, hand back the already measured padding window."
      },
      {
        "match": "for (phdr_index = 0; (uint)phdr_index < (uint)(ushort)elf_info->phdr_count; phdr_index = phdr_index + 1) {",
        "match_type": "wildcard",
        "comment": "Walk every PF_W|PF_R PT_LOAD, aligning `[start,end)` and tracking the segment whose aligned end extends the farthest."
      },
      {
        "match": "padding_length = (long)cached_data_start - (long)segment_mem_end_ptr;",
        "match_type": "wildcard",
        "comment": "Once the candidate is known, compute the file-backed end, the `.bss` tail, and the padding up to the next page boundary."
      }
    ]
  },
  "elf_get_got_symbol": {
    "plate": "Mirrors `elf_get_plt_symbol` but targets the main RELA table. It requires feature bit 2 (RELA metadata), then calls\n`elf_get_reloc_symbol` with relocation type 6 (R_X86_64_GLOB_DAT) to retrieve the writable GOT slot for the\nrequested import. Failure means the module never imported the symbol through a standard GOT relocation.\n",
    "inline": [
      {
        "match": "if (((elf_info->feature_flags & 2) != 0) && (elf_info->rela_reloc_count != 0)) {",
        "match_type": "wildcard",
        "comment": "Skip immediately when the RELA table was missing."
      },
      {
        "match": "symbol_slot = elf_get_reloc_symbol",
        "match_type": "wildcard",
        "comment": "Reuse the generic helper with R_X86_64_GLOB_DAT to land on the writable GOT slot."
      }
    ]
  },
  "elf_get_plt_symbol": {
    "plate": "Looks up the PLT thunk for a given symbol by delegating to `elf_get_reloc_symbol` with relocation type 7\n(R_X86_64_JUMP_SLOT). It first verifies that PLT relocations exist (feature bit 1 plus a non-zero count) and then\nreturns the GOT/PLT entry that will be rewritten during hook installation. NULL indicates the table was absent or\nthe symbol never appeared in it.\n",
    "inline": [
      {
        "match": "if (((elf_info->feature_flags & 1) != 0) && (elf_info->plt_reloc_count != 0)) {",
        "match_type": "wildcard",
        "comment": "Fast fail when the binary never exposed PLT relocation metadata."
      },
      {
        "match": "symbol_slot = elf_get_reloc_symbol",
        "match_type": "wildcard",
        "comment": "Delegate to the generic helper with R_X86_64_JUMP_SLOT so we capture the PLT thunk."
      }
    ]
  },
  "elf_get_reloc_symbol": {
    "plate": "Generic helper that scans any relocation array for undefined symbols of a particular relocation type (e.g.,\nGOT vs. PLT) and encoded name. It walks `num_relocs`, enforces the requested `reloc_type`, confirms the symbol is\nreally an import (`st_shndx == 0`), and hashes the name via `get_string_id` before comparing it to\n`encoded_string_id`. Matching entries return the relocated slot (`elfbase + r_offset`) so callers can patch GOT/PLT\nentries in place. Every lookup is gated by `secret_data_append_from_address` so relocation edits only happen while\nthe secret-data recorder is active.\n",
    "inline": [
      {
        "match": "telemetry_ok = secret_data_append_from_address(",
        "match_type": "wildcard",
        "comment": "Relocation hunts stay tied to the secret-data log; skip the scan entirely when telemetry fails."
      },
      {
        "match": "if ((((relocs->r_info & 0xffffffff) == reloc_type) &&",
        "match_type": "wildcard",
        "comment": "Filter on the relocation type and insist the associated symbol is an unresolved import before hashing the name."
      },
      {
        "match": "return elf_info->elfbase->e_ident + relocs->r_offset;",
        "match_type": "wildcard",
        "comment": "Hand the caller the writable relocation slot (module base + `r_offset`) once a match is found."
      }
    ]
  },
  "elf_get_rodata_segment": {
    "plate": "Locates and caches the first PF_R PT_LOAD segment that begins strictly after the executable code. The helper logs a `secret_data_append_from_call_site` breadcrumb, returns the cached `rodata_segment_start/size` when they already exist, and otherwise asks `elf_get_code_segment` for the `[text_start, text_end)` window. It then iterates every program header, converts each PF_R-only mapping into a runtime pointer, page-aligns `[segment_start, segment_end)`, and tracks the lowest candidate whose aligned base sits at or beyond `text_end`. The winning base/size are recorded inside `elf_info_t` and handed back via `pSize` so later string scans and RELRO checks can reuse the same rodata window.",
    "inline": [
      {
        "match": "telemetry_ok = secret_data_append_from_call_site(",
        "match_type": "wildcard",
        "comment": "Abort immediately when the logger refuses the breadcrumb—rodata scans must mirror the secret-data log."
      },
      {
        "match": "if (segment_start_ptr != (void *)0x0) {",
        "match_type": "wildcard",
        "comment": "Subsequent callers simply reuse the cached base/size instead of re-walking the PT_LOAD list."
      },
      {
        "match": "for (phdr_index = 0; (uint)phdr_index < (uint)(ushort)elf_info->phdr_count; phdr_index = phdr_index + 1) {",
        "match_type": "wildcard",
        "comment": "Look for PF_R-only PT_LOAD entries whose aligned base lands beyond `.text`, keeping the lowest such segment."
      }
    ]
  },
  "elf_parse": {
    "plate": "Initialises an `elf_info_t` from an in-memory ELF header: zeroes every field, records the lowest PT_LOAD virtual address, locates the PT_DYNAMIC segment, and caches pointers to the strtab, symtab, relocation tables (PLT, RELA, RELR), GNU hash buckets, version records, and GNU_RELRO metadata. Each derived pointer is validated with `elf_contains_vaddr`, and the parser keeps feature bits synchronized so later helpers know which tables were present.\n\nIt enforces invariants such as \"only one PT_GNU_RELRO segment\", derives the number of dynamic entries, honours BIND_NOW/RELR/versym toggles, and refuses to trust any header that leaves the module boundaries.",
    "inline": [
      {
        "match": "for (dynamic_phdr_idx = 0x3e; dynamic_phdr_idx != 0; dynamic_phdr_idx = dynamic_phdr_idx + -1)",
        "match_type": "wildcard",
        "comment": "Clear the cached `elf_info_t` with a fixed-count wipe so partially parsed state never leaks across runs."
      },
      {
        "match": "range_valid = elf_contains_vaddr(elf_info,vaddr,phnum,4);",
        "match_type": "wildcard",
        "comment": "Reject the ELF immediately if the PT_DYNAMIC payload does not live entirely inside readable memory."
      },
      {
        "match": "if (strtab_base <= ehdr) {",
        "match_type": "wildcard",
        "comment": "Convert relative pointers (PIE layout) into process addresses before recording them in `elf_info_t`."
      },
      {
        "match": "if (((((elf_info->plt_relocs == (Elf64_Rela *)0x0) ||",
        "match_type": "wildcard",
        "comment": "Every relocation/metadata pointer gets revalidated before harvesting the GNU hash bucket/chain tables."
      }
    ]
  },
  "elf_symbol_get": {
    "plate": "Symbol resolver that trusts the GNU hash table the loader extracted earlier. After emitting secret-data telemetry it iterates every\nbucket, validates the bucket and chain addresses, and replays the GNU hash walk to pull `Elf64_Sym` entries out of `.dynsym`.\nCandidates must have both `st_value` and `st_shndx` populated and their names get hashed (via `get_string_id`) so the caller’s\nencoded id can be matched without copying strings around.\n\nWhen a version id is supplied the helper also consults `.gnu.version`/`.gnu.version_d`: it reads the `versym` slot, walks the\nlinked `Elf64_Verdef` list, and compares the version string id. Returning NULL means the symbol/version was missing, the module\nnever exposed the GNU hash + version tables, or one of the string/relocation pointers failed validation mid-walk.",
    "inline": [
      {
        "match": "*range_ok = secret_data_append_from_call_site((secret_data_shift_cursor_t)0x58,0xf,3,FALSE);*",
        "match_type": "wildcard",
        "comment": "Emit a secret-data breadcrumb before touching the GNU hash tables so symbol hunts show up in the telemetry log."
      },
      {
        "match": "if ((sym_entry->st_value != 0) && (sym_entry->st_shndx != 0)) {",
        "match_type": "wildcard",
        "comment": "Skip undefined/imported symbols—the resolver only accepts entries that already have a concrete value and section."
      },
      {
        "match": "if (((elf_info->feature_flags & 0x18) == 0x18) && ((versym_index & 0x7ffe) != 0)) {",
        "match_type": "wildcard",
        "comment": "When versioning metadata exists, walk `.gnu.version_d` to make sure the caller’s requested version string also matches."
      }
    ]
  },
  "elf_symbol_get_addr": {
    "plate": "Convenience layer on top of `elf_symbol_get`: look up the symbol, make sure it is defined (both `st_value` and `st_shndx` are\nnon-zero), and then turn the symbol value into a process address by adding it to `elf_info->elfbase`. Returning NULL indicates\neither the symbol does not exist or it represents an import/resolver stub that lacks a concrete address.",
    "inline": [
      {
        "match": "sym_entry = elf_symbol_get(elf_info,encoded_string_id,0);",
        "comment": "Delegate to the GNU-hash resolver first—there’s nothing to add if the lookup already failed."
      },
      {
        "match": "if ((sym_entry->st_value == 0) || (sym_entry->st_shndx == 0)) {",
        "match_type": "wildcard",
        "comment": "Undefined or import-only records never produce a concrete address, so bail out immediately."
      },
      {
        "match": "sym_entry = (Elf64_Sym *)(elf_info->elfbase->e_ident + sym_entry->st_value);",
        "match_type": "wildcard",
        "comment": "Add the symbol value to the module base to obtain its process address."
      }
    ]
  },
  "extract_payload_message": {
    "plate": "Searches an sshbuf blob for either `\"ssh-rsa-cert-v01@openssh.com\"` or `\"rsa-sha2-256\"`, using the surrounding length\nfields (all big-endian) to walk the serialized key structure. It validates every intermediate length (capping them at\n0x10000), ensures the proposed modulus chunk fits within the caller-provided buffer, and rewrites `sshbuf->d` to point\ndirectly at that modulus blob. The extracted length is returned via `out_payload_size` so the decryptor knows exactly\nhow many bytes to peel off.",
    "inline": [
      {
        "match": "if (CARRY8((ulong)sshbuf_start,sshbuf_size)) {",
        "comment": "Reject buffers whose base-plus-size would wrap the address space—those pointers would leave the sshbuf view."
      },
      {
        "match": "alg_match_cursor = sshbuf_start + cursor_offset;",
        "comment": "Slide a search window across the buffer, preferring the cert algorithm tag and falling back to the RSA-SHA2 string."
      },
      {
        "match": "field_length = *(uint *)(alg_match_cursor + -8);",
        "comment": "Use the big-endian length that precedes the algorithm name to clamp the serialized key record."
      },
      {
        "match": "cursor_offset = c_strnlen((char *)alg_match_cursor,window_size);",
        "comment": "Treat the algorithm string as bounded input so we never read past the declared record tail."
      },
      {
        "match": "sshbuf_data->d = (u8 *)modulus_cursor;",
        "comment": "Point the caller's sshbuf directly at the modulus blob and report its length via `out_payload_size`."
      }
    ]
  },

  "fake_lzma_alloc": {
    "plate": "Companion to `fake_lzma_free` that turns the liblzma allocation API into a symbol resolver. The `opaque` parameter is treated as\nan `elf_info_t *`, the requested `size` is reinterpreted as an `EncodedStringId`, and it simply returns whatever\n`elf_symbol_get_addr()` produces. The `nmemb` argument is ignored because the helper is never asked to allocate real memory—it\nonly masquerades as an allocator long enough to bootstrap symbol lookups inside ld.so.",
    "inline": [
      {
        "match": "symbol_addr = elf_symbol_get_addr((elf_info_t *)opaque,(EncodedStringId)size);",
        "match_type": "wildcard",
        "comment": "Treat `opaque` as `elf_info_t *` and `size` as the EncodedStringId the loader wants resolved."
      }
    ]
  },
  "fake_lzma_free": {
    "plate": "No-op placeholder that exists solely to satisfy the liblzma allocator interface the implant exposes. The loader wires this stub into `lzma_allocator.free` until it can swap in the genuine host callbacks, so any invocation is guaranteed to do nothing other than prove that the fake allocator is still active.\n\nHaving an inert body keeps the import surface small while still exporting a correctly typed symbol, and it gives the runtime a reliable indicator that a caller incorrectly tried to free memory through the bootstrap allocator.",
    "inline": [
      {
        "match": "return;",
        "comment": "Both `opaque` and `ptr` are ignored on purpose—the stub merely signals that the fake allocator table is still installed.",
        "occurrence": 1
      }
    ]
  },
  "fd_read": "Wraps libc’s read with retry logic. It refuses to run without both `read` and `__errno_location`, loops on EINTR, and aborts\nwith -1 when the helper sees EOF before the requested byte count. Successful reads consume the entire length so callers can\ntreat any non-zero return as \"buffer filled\".",
  "fd_write": "Mirror of `fd_read`: it requires valid write/errno pointers, retries on EINTR, and treats short writes as fatal so callers\neither send the entire buffer or receive -1. It is the plumbing used whenever the implant forges monitor messages.",
  "find_add_instruction_with_mem_operand": {
    "plate": "ADD predicate for register-to-memory increments.\nIt wipes a scratch decoder whenever `dctx` is NULL, advances by a single byte on failed decodes, and insists the instruction stream produces opcode `0x103` with a memory ModRM form.\nIf `mem_address` is set it also requires DF2 plus a RIP-relative displacement that recomputes to that pointer before returning TRUE.",
    "inline": [
      {
        "match": "for (ctx_clear_idx = 0x16; ctx_clear_idx != 0; ctx_clear_idx = ctx_clear_idx + -1)",
        "match_type": "wildcard",
        "comment": "Keep the scratch decoder clean so each scan starts from a known state."
      },
      {
        "match": "if ((((add_found != FALSE) && (*(int *)(dctx->opcode_window + 3) == 0x103)) &&",
        "match_type": "wildcard",
        "comment": "Only accept opcode 0x103 (ADD r/m64,r64) when it actually targets memory."
      },
      {
        "match": "((mem_address == (void *)0x0 ||",
        "match_type": "wildcard",
        "comment": "Optionally demand DF2 plus the RIP-relative displacement that lands on the requested pointer.",
        "placement": "after"
      }
    ]
  },
  "find_addr_referenced_in_mov_instruction": {
    "plate": "Given a `StringXrefId`, this helper looks up the owning function span and walks it for MOV-load references.\nIt repeatedly calls `find_instruction_with_mem_operand_ex` (opcode 0x10b), ignores 64-bit/REX.W MOVs, and either recomputes the absolute address (handling RIP-relative displacements) or bails out if the candidate lacked DF2 and the caller never supplied a range.\nThe first pointer that lands inside `[mem_range_start, mem_range_end)` is returned; everything else yields NULL.",
    "inline": [
      {
        "match": "for (ctx_clear_idx = 0x16; ctx_clear_idx != 0; ctx_clear_idx = ctx_clear_idx + -1)",
        "match_type": "wildcard",
        "comment": "Reset the scratch decoder we hand to `find_instruction_with_mem_operand_ex`."
      },
      {
        "match_regex": "func_cursor = \\(u8 \\*\\)\\(&refs->xcalloc_zero_size\\)\\[id\\]\\.func_start;",
        "match_type": "regex",
        "comment": "Fetch the cached `[func_start, func_end)` range associated with this string ID."
      },
      {
        "match": "if (((byte)scratch_ctx.prefix.decoded.rex & 0x48) != 0x48)",
        "match_type": "wildcard",
        "comment": "Ignore MOVs that flip REX.W; the string tables we track always use 32-bit pointers.",
        "placement": "after"
      },
      {
        "match": "if ((scratch_ctx.prefix.decoded.flags2 & 1) == 0)",
        "match_type": "wildcard",
        "comment": "Without DF2 there is no displacement to recompute, so abort unless the caller insisted on a range.",
        "placement": "after"
      },
      {
        "match": "if (((uint)scratch_ctx.prefix.decoded.modrm & 0xff00ff00) == 0x5000000)",
        "match_type": "wildcard",
        "comment": "RIP-relative ModRM forms need the extra `instruction + instruction_size` correction.",
        "placement": "after"
      },
      {
        "match": "if ((mem_range_start <= candidate_addr) && (candidate_addr <= (u8 *)((long)mem_range_end + -4)))",
        "match_type": "wildcard",
        "comment": "Treat the range as inclusive of the start and exclusive of the four-byte tail so we only return pointers inside the blob.",
        "placement": "after"
      }
    ]
  },
  "find_call_instruction": {
    "plate": "Appends a secret-data breadcrumb, zeros the caller-supplied (or temporary) `dasm_ctx_t`, and walks `[code_start, code_end)` with `x86_dasm`. Decode failures advance by one byte, while successes advance by `instruction_size` until the normalised CALL opcode (`0x168`) is seen. When `call_target` is non-NULL it further requires the rel32 destination (`instruction + instruction_size + imm_signed`) to match before returning TRUE. On success the context still describes the CALL so callers can immediately rewrite or inspect it.",
    "inline": [
      {
        "match_regex": "secret_data_append_from_address\\(\\(void \\*\\)0x0,\\(secret_data_shift_cursor_t\\)0x81,4,7\\);",
        "match_type": "regex",
        "comment": "Telemetry hooks track each pointer scan so the secret-data log mirrors our search paths.",
        "placement": "after"
      },
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Scratch decoders get wiped between attempts so prefixes/immediates never leak."
      },
      {
        "match": "while (* < *) {",
        "match_type": "wildcard",
        "comment": "Decode one instruction at a time until a CALL (optionally to call_target) appears."
      },
      {
        "match": "if (* == FALSE) {",
        "match_type": "wildcard",
        "comment": "Decoder failed, so retry one byte later just like a pattern scan."
      },
      {
        "match": "* = * + *->instruction_size;",
        "match_type": "wildcard",
        "comment": "Successful decodes hop past the instruction so the next iteration sees the following op."
      }
    ]
  },
  "find_dl_audit_offsets": "Coordinates the entire ld.so reconnaissance pass. It resolves the necessary EC/EVP helpers via the fake allocator, copies\n`_dl_audit_symbind_alt`’s address/size out of ld.so, and uses `find_link_map_l_name` to compute the displacement between the\ncached and live link_map entries. With that offset it invokes `find_dl_naudit` to capture the `_dl_naudit`/`_dl_audit` pointers\nand `find_link_map_l_audit_any_plt` to learn where the audit bit lives. Finally it copies libcrypto’s basename into\n`hooks->ldso_ctx` so the forged `l_name` string looks correct. Any failure frees the temporary imports and aborts the audit-hook\ninstall path.",
  "find_dl_naudit": "Harvests the `_dl_naudit` counter and `_dl_audit` pointer from ld.so so the loader can toggle audit modules. After resolving\n`DSA_get0_pqg`, `DSA_get0_pub_key`, and `EVP_MD_CTX_free` via the fake allocator, it finds `rtld_global_ro`, searches for the\n`GLRO(dl_naudit)` string reference, and decodes the MOV that loads that slot. The same memory address is then matched inside\n`_dl_audit_symbind_alt`; if the MOV/TEST pair is found and the slot is still zero, the function records the `_dl_naudit` and\n`_dl_audit` pointers in `hooks->ldso_ctx`. Any deviation or pre-existing audit module aborts the attempt.",
  "find_function": {
    "plate": "Wraps `find_function_prologue` to recover the full function bounds surrounding an instruction. When `func_start` is requested it walks backward from `code_start` toward `search_base`, probing each byte with the prologue helper until it finds a landing pad; hitting `search_base` without success aborts. When `func_end` is requested it scans forward until the next prologue (or `code_end`) and uses that offset as the end marker. Successful runs populate whichever out pointers the caller requested so later passes can reason about bounded regions instead of raw addresses.",
    "inline": [
      {
        "match": "while ((* < * &&",
        "match_type": "wildcard",
        "comment": "Walk backward byte-by-byte until we rediscover a prologue or hit the supplied floor."
      },
      {
        "match": "for (; * < * + -4; * = * + 1) {",
        "match_type": "wildcard",
        "comment": "Scan forward until the next landing pad so callers know where the function stops."
      }
    ]
  },
  "find_function_prologue": {
    "plate": "Recognises CET-style prologues. In `FIND_ENDBR64` mode it zeroes a scratch `dasm_ctx_t`, asks `x86_dasm` to decode at `code_start`, requires the normalised opcode to be ENDBR64, and only succeeds if the ENDBR padding ends on a 16-byte boundary (optionally returning the instruction immediately after the pad through `output`). Legacy mode skips the decoder and reuses `is_endbr64_instruction` with the simple mask; when it hits it reports the exact byte it just tested so callers can keep walking until they find the landing pad they need.",
    "inline": [
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Zero a scratch decoder context so we can peek at the opcode without mutating caller state."
      },
      {
        "match_regex": "if \\(\\(\\([^)]* != FALSE\\) && \\(\\*\\(u32 \\*\\)&[^.]+\\.opcode_window\\[3] == 3999\\)\\) &&",
        "match_type": "regex",
        "comment": "Valid ENDBR pads must end on a 16-byte boundary; optionally hand the caller the next byte.",
        "placement": "after"
      },
      {
        "match": "if (* != FALSE) {",
        "match_type": "wildcard",
        "comment": "Legacy mode simply reports the matching byte so the caller can keep searching."
      }
    ]
  },
  "find_instruction_with_mem_operand": {
    "plate": "Two-stage absolute-pointer predicate shared by the recon helpers.\nIt prefers LEA results (via `find_lea_instruction_with_mem_operand`) so RIP-relative references never touch memory, and only falls back to the MOV-load predicate (`find_instruction_with_mem_operand_ex` with opcode `0x10b`) once the LEA attempt fails.\nTRUE means `dctx` is still positioned on the instruction that materialised `mem_address`.",
    "inline": [
      {
        "match": "match_found = find_instruction_with_mem_operand_ex(*",
        "match_type": "wildcard",
        "comment": "Fallback to MOV loads when the LEA scan cannot find the requested pointer."
      }
    ]
  },
  "find_instruction_with_mem_operand_ex": {
    "plate": "Generic predicate used by the pointer scanners to match one opcode that actually touches memory.\nIt logs the probe via `secret_data_append_from_call_site`, wipes a scratch decoder when the caller passes NULL, and then slides a one-byte window from `code_start` to `code_end` until `x86_dasm` decodes the requested opcode.\nEach hit still has to present a memory ModRM form and, when `mem_address` is provided, set DF2 and produce a RIP-relative displacement that recomputes to that address before the helper returns TRUE with `dctx` left on the instruction.",
    "inline": [
      {
        "match": "search_ok = secret_data_append_from_call_site(*",
        "match_type": "wildcard",
        "comment": "Feed the instrumentation/log buffer so we can recover which opcode sweep triggered this search."
      },
      {
        "match": "for (ctx_clear_idx = 0x16; ctx_clear_idx != 0; ctx_clear_idx = ctx_clear_idx + -1)",
        "match_type": "wildcard",
        "comment": "Zero the scratch decoder whenever the caller does not supply one."
      },
      {
        "match": "for (; code_start < code_end; code_start = code_start + 1)",
        "match_type": "wildcard",
        "comment": "Slide a single-byte window forward so failed decodes simply advance to the next offset."
      },
      {
        "match": "if ((((search_ok != FALSE) && (*(int *)(dctx->opcode_window + 3) == opcode)) &&",
        "match_type": "wildcard",
        "comment": "Require the decoded opcode plus a memory ModRM form before considering the DF2/RIP tests."
      },
      {
        "match": "((mem_address == (void *)0x0 ||",
        "match_type": "wildcard",
        "comment": "When the caller provides a target pointer, insist DF2 is set and the RIP-relative recomputation lands exactly on it.",
        "placement": "after"
      }
    ]
  },
  "find_lea_instruction": {
    "plate": "Telemetry-backed search for LEA instructions that materialise a specific displacement.\nIt emits `secret_data_append_from_call_site`, clears the stack-resident decoder via the `ctx_clear_idx` / `ctx_clear_cursor` loop (the odd `ctx_stride_sign` artefact simply keeps the stride positive), and then decodes forward one byte at a time until it sees opcode `0x10d` with DF2 reporting a plain displacement operand.\nEither `displacement` or its negated twin qualifies, letting mirrored scans succeed, and the populated `lea_ctx` is left in place so callers can immediately interrogate the operands.",
    "inline": [
      {
        "match": "* = secret_data_append_from_call_site((secret_data_shift_cursor_t)0x7c,5,6,FALSE);",
        "match_type": "wildcard",
        "comment": "Breadcrumb the scan so we know which helper touched each code window.",
        "placement": "after"
      },
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Reset the decoder context between attempts (the stride sign flip is a compiler artefact)."
      },
      {
        "match": "* = x86_dasm(&*,*,*);",
        "match_type": "wildcard",
        "comment": "Decode byte-by-byte until a LEA with a bare displacement materialises."
      },
      {
        "match_regex": "if .*\\(\\*\\(u32 \\*\\)&[^.]+\\.opcode_window\\[3] == 0x10d\\)",
        "match_type": "regex",
        "comment": "Accept mirrored displacements so searches anchored at ±delta both succeed.",
        "placement": "after"
      }
    ]
  },
  "find_lea_instruction_with_mem_operand": {
    "plate": "LEA finder that insists the instruction truly references memory and, optionally, a concrete absolute address.\nAfter logging the call site via `secret_data_append_from_call_site` it zeroes the fallback decoder with the `ctx_clear_idx` / `ctx_clear_cursor` pair when the caller passed NULL, then decodes forward one byte at a time until it sees opcode `0x10d`, REX.W set, and a ModRM mode that touches memory.\nIf `mem_address` is non-null it recomputes the RIP-relative target (`instruction + instruction_size + mem_disp`) and only succeeds on an exact match; otherwise any qualifying LEA returns TRUE with `dctx` still on the instruction.",
    "inline": [
      {
        "match": "* = secret_data_append_from_call_site((secret_data_shift_cursor_t)0x1c8,0,0x1e,FALSE);",
        "match_type": "wildcard",
        "comment": "Log the caller so later telemetry can explain which helper touched a given memory range.",
        "placement": "after"
      },
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Zero the scratch decoder so we only evaluate the LEA we just decoded."
      },
      {
        "match": "* = x86_dasm(*,*,*);",
        "match_type": "wildcard",
        "comment": "Keep decoding until we see a 64-bit LEA that genuinely targets memory."
      },
      {
        "match_regex": "if .*\\(\\*\\(int \\*\\)\\([^)]*opcode_window \\+ 3\\) == 0x10d\\)",
        "match_type": "regex",
        "comment": "Optional RIP target comparison lets callers lock onto a single absolute pointer.",
        "placement": "after"
      }
    ]
  },
  "find_link_map_l_audit_any_plt": "Primes an `instruction_search_ctx_t` before invoking the bitmask helper. It sweeps `_dl_audit_symbind_alt` for the LEA that\nmaterialises `link_map::l_name` using the caller-provided displacement, records which registers capture the pointer versus the\nmask, initialises the register filters/output buffers, and then calls `find_link_map_l_audit_any_plt_bitmask`. Success means\nboth the byte offset and AND mask are now stored in `hooks->ldso_ctx`; failure either means the pattern never appeared or the\nbit was already non-zero.",
  "find_link_map_l_audit_any_plt_bitmask": "Takes the displacement from `find_link_map_l_name` and hunts for the byte and mask that back ld.so’s `link_map::l_audit_any_plt`\nflag. It temporarily resolves `EVP_DecryptInit_ex` and libc’s `getuid`, decodes `_dl_audit_symbind_alt` with `x86_dasm`, and\ntracks which register holds the computed pointer. Once it sees the MOV-from-link_map followed by a TEST/BT it validates that the\nmask is a single set bit, records the absolute address in `hooks->ldso_ctx.sshd_link_map_l_audit_any_plt_addr`, stores the byte-\nwide mask, and insists the bit is still clear; otherwise the helper sets the search context’s `result` flag and bails out.",
  "find_link_map_l_name": "Locates the live `link_map::l_name` byte inside ld.so and gathers the libc/libcrypto imports needed later in the run. It\npiggybacks on the fake `lzma_alloc` resolver to look up `exit`, `setlogmask`, `setresgid`, `setresuid`, `system`, `shutdown`,\nand `BN_num_bits`, then walks the cached liblzma link_map snapshot inside the binary until it finds the entry whose RELRO tuple\nmatches the running liblzma image. The resulting displacement becomes both `*libname_offset` and the pointer used to index\n`hooks->ldso_ctx.libcrypto_l_name`, and the helper double-checks that `_dl_audit_symbind_alt` references the same offset so\nlater code can safely rewrite the `l_name` field when posing as an audit module.",
  "find_mov_instruction": {
    "plate": "MOV-only variant of the pointer scan.\nIf `dctx` is NULL it wipes the on-stack `scratch_ctx` with the `ctx_clear_idx` / `ctx_clear_cursor` walkers and decodes into that temporary, retrying from the next byte when a decode fails.\nSuccessful decodes advance by `instruction_size` and must show a memory↔register ModRM form plus opcode `0x10b` (load) or `0x109` (store) per `load_flag`; loads also require the decoded REX.W bit to match `is_64bit_operand`. On success the populated decoder is left on the MOV so pointer-tracking helpers can read the operands immediately.",
    "inline": [
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Keep the decoder context pristine so predicates only see the current instruction."
      },
      {
        "match": "* = * + 1;",
        "match_type": "wildcard",
        "comment": "Retry at the next byte boundary when the decoder chokes on garbage."
      },
      {
        "match": "if ((((*->prefix).decoded.modrm.modrm_word & 0xff00ff00) == 0x5000000) &&",
        "match_type": "wildcard",
        "comment": "Enforce the memory↔register ModRM form and the caller-requested operand width.",
        "placement": "after"
      }
    ]
  },
  "find_mov_lea_instruction": {
    "plate": "Hybrid MOV/LEA predicate that underpins the pointer scanners.\nIt zeroes the fallback decoder (again via `ctx_clear_idx` / `ctx_clear_cursor`) whenever the caller passes NULL, marches forward either by one byte (decode failure) or by `instruction_size` (success), and insists the ModRM form exposes a memory operand.\nOpcode `0x10d` (LEA) always qualifies, otherwise it requires the MOV opcode that matches `load_flag` (`0x10b` for loads, `0x109` for stores) and enforces that REX.W matches `is_64bit_operand` unless the caller is searching for stores. Returning TRUE leaves `dctx` describing the matching instruction for downstream helpers.",
    "inline": [
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Keep the scratch decoder pristine so stale state never influences later scans."
      },
      {
        "match": "* = * + 1;",
        "match_type": "wildcard",
        "comment": "Failed decodes advance byte-by-byte until the next valid opcode appears."
      },
      {
        "match": "if ((((*->prefix).decoded.modrm.modrm_word & 0xff00ff00) == 0x5000000) &&",
        "match_type": "wildcard",
        "comment": "Only accept true memory operands plus the caller-requested width (unless we are hunting stores).",
        "placement": "after"
      }
    ]
  },
  "find_reg2reg_instruction": {
    "plate": "Requires a caller-supplied decoder context and walks forward one instruction at a time until it sees a register-only transfer.\nIt rejects every decode that carries lock/rep prefixes, sets REX.W/B, or uses a ModRM mode other than 3, and then checks whether the opcode is either MOV reg↔reg or one of the arithmetic-immediate opcodes addressed via `opcode_lookup_index = opcode - 0x81` (the precomputed bitmask tracks the admissible subset).\nDecode failures or reaching `code_end` return FALSE; success leaves `dctx` still pointing at the qualifying instruction so register-propagation helpers know the value never touched memory.",
    "inline": [
      {
        "match_regex": "if \\([^=]+== \\(dasm_ctx_t \\*\\)0x0\\) \\{",
        "match_type": "regex",
        "comment": "Unlike the other helpers we require a persistent decoder supplied by the caller.",
        "placement": "after"
      },
      {
        "match_regex": "if .*\\(\\*\\(uint \\*\\)\\([^)]*opcode_window \\+ 3\\) & 0xfffffffd\\) == 0x109",
        "match_type": "regex",
        "comment": "Accept MOV reg↔reg or whitelisted arithmetic immediates with no prefixes and ModRM mode 3."
      },
      {
        "match": "* = *->instruction + *->instruction_size;",
        "match_type": "wildcard",
        "comment": "Skip past any instruction that still touches memory or flips prefixes we care about."
      }
    ]
  },
  "find_string_reference": {
    "plate": "Bootstraps the LEA pointer scan used by the string-reference tables.\nIt wipes a scratch `dasm_ctx_t`, runs `find_lea_instruction_with_mem_operand` across `[code_start, code_end)`, and only considers hits whose computed pointer equals `str`.\nSuccess returns the LEA's address so callers can treat that instruction as the reference site; otherwise NULL bubbles up.",
    "inline": [
      {
        "match": "for (ctx_clear_idx = 0x16; ctx_clear_idx != 0; ctx_clear_idx = ctx_clear_idx + -1)",
        "match_type": "wildcard",
        "comment": "Reset the scratch decoder before handing it to the LEA searcher."
      },
      {
        "match": "lea_anchor = scratch_ctx.instruction;",
        "match_type": "wildcard",
        "comment": "Return the successful LEA's address so string walkers can anchor the xref there.",
        "placement": "after"
      }
    ]
  },
  "get_elf_functions_address": {
    "plate": "Same pattern for the `elf_functions_t` dispatch table: start from the relocation-safe sentinel (`elf_functions_offset` lives near `fake_lzma_allocator_offset` in `.data`) and advance 12 struct slots to arrive at the live table. The convoluted pointer math lets the object carry offsets instead of absolute addresses, which keeps the relocation surface tiny while still giving the loader a stable way to reach its helper vtable.",
    "inline": [
      {
        "match": "table_cursor = (elf_functions_t *)fake_lzma_allocator_offset;",
        "comment": "Start from the relocation-safe sentinel that ships next to the fake allocator blob so the pointer stays valid pre-patch."
      },
      {
        "match": "for (slot_idx = 0; slot_idx < 0xc; slot_idx = slot_idx + 1) {",
        "comment": "Advance 12 struct slots (~0x160 bytes) to land on the live dispatch table populated by the loader."
      }
    ]
  },
  "get_lzma_allocator": {
    "plate": "Wraps `get_lzma_allocator_address()` so the loader can recover the fake allocator blob at runtime and expose only its embedded `lzma_allocator` callbacks. This lets callers pass liblzma-compatible alloc/free hooks downstream while the surrounding bookkeeping (and `opaque` pointer to the implant's `elf_info_t`) stays hidden.",
    "inline": [
      {
        "match": "fake_allocator = get_lzma_allocator_address();",
        "comment": "Resolve the relocated fake allocator blob each time; the sentinel math accounts for where the loader staged it."
      },
      {
        "match": "return &fake_allocator->allocator;",
        "comment": "Publish just the nested liblzma callbacks so outside callers never learn about the rest of the blob."
      }
    ]
  },
  "get_lzma_allocator_address": {
    "plate": "Manual pointer arithmetic that recovers the runtime address of the fake `fake_lzma_allocator_t` blob without requiring relocatable absolute addresses. The compiler emits a sentinel (`fake_lzma_allocator`) followed by padding, so this helper starts at that symbol and steps through the struct 12 times, effectively adding the baked-in 0x160-byte offset that lands on the real allocator instance the loader populated at build time.",
    "inline": [
      {
        "match": "allocator_cursor = (fake_lzma_allocator_t *)fake_lzma_allocator;",
        "comment": "Start from the relocation-safe sentinel the compiler left embedded right before the real allocator blob."
      },
      {
        "match": "for (slot_idx = 0; slot_idx < 0xc; slot_idx = slot_idx + 1) {",
        "comment": "Walk 12 struct slots (~0x160 bytes) forward to reach the live allocator instance stage two writes."
      }
    ]
  },
  "get_string_id": "Maps runtime strings to EncodedStringId identifiers without shipping plaintext literals. Each call logs itself via\nsecret_data_append_from_address, clamps the scan to 0x2c bytes, and walks two packed bitmaps (_Lcrc64_clmul_1+0x760 and the\nstring_action_data trie) while repeatedly calling count_bits to compute the next child index. It returns the encoded ID when it\nreaches a terminal node or 0 when the bytes miss the trie, and is used to find SSH banner strings during sshd heuristics.",
  "get_tls_get_addr_random_symbol_got_offset": "Seeds `ctx->got_ctx.tls_got_entry` and `ctx->got_ctx.got_base_offset` with the canned values associated with the fake `__tls_get_addr`\nsymbol. The loader uses those numbers as the starting point for `update_got_address`, which refines them into the concrete GOT\nentry address.",
  "hook_EVP_PKEY_set1_RSA": "Tap point for EVP_PKEY_set1_RSA so the backdoor sees every RSA handle even when the decrypt hook never fires. It simply calls\nrun_backdoor_commands on the key and then invokes the preserved OpenSSL routine.",
  "hook_RSA_get0_key": "Mirrors the same idea for RSA_get0_key: every consumer that asks OpenSSL for the modulus/exponent triggers run_backdoor_commands\nfirst, letting the implant inspect/track the RSA handle before delegating to the original function.",
  "hook_RSA_public_decrypt": "Replacement for RSA_public_decrypt: it ensures the PLT pointer is resolved, hands the RSA key to run_backdoor_commands (passing\na do_orig flag by reference), and either returns the backdoor’s result or forwards the call to the genuine OpenSSL symbol\ndepending on whether the dispatcher consumed the ciphertext.",
  "init_elf_entry_ctx": {
    "plate": "Primes an `elf_entry_ctx_t` before the IFUNC resolvers run. It latches the `cpuid_random_symbol` anchor, copies the resolver's saved return address out of `ctx->resolver_frame[3]` as the cpuid GOT slot, replays `update_got_offset`/`update_cpuid_got_index`, and clears `tls_got_entry` so the later GOT patch re-resolves the TLS entry before grafting in the malicious cpuid stub.",
    "inline": [
      {
        "match": "ctx->cpuid_random_symbol_addr = &_Lrc_read_destroy;",
        "comment": "Record the benign cpuid resolver's address so GOT math has a stable anchor."
      },
      {
        "match": "(ctx->got_ctx).cpuid_got_slot = (void *)ctx->resolver_frame[3];",
        "comment": "Lift the resolver's saved return address (slot 3) as the GOT slot that will be patched."
      },
      {
        "match": "update_got_offset(ctx);",
        "comment": "Recompute the GOT base offset before the hook splices anything into the table."
      },
      {
        "match": "update_cpuid_got_index(ctx);",
        "comment": "Refresh the cpuid GOT index while the resolver's frame is still intact."
      },
      {
        "match": "(ctx->got_ctx).tls_got_entry = (void *)0x0;",
        "comment": "Clear the cached TLS entry so the impending hook forces a new `__tls_get_addr` resolution."
      }
    ]
  },
  "init_hooks_ctx": "Primes a transient `backdoor_hooks_ctx_t` before stage two patches the GOT. It always points `hooks_data_addr` at the\n`hooks_data` blob baked into liblzma, zeros the scratch flags, and, when `ctx->shared` is still NULL, drops in the static hook\nentry points (`backdoor_symbind64`, the RSA shims, and the mm_* monitor hooks) before returning 0x65 so the caller can retry\nafter the shared globals are published. Once the shared block exists it simply returns 0, signalling that the structure now\ninherits every pointer from the shared globals.",
  "init_imported_funcs": "Sanity-checks the OpenSSL import table before the hooks are allowed to run. It requires `resolved_imports_count` to equal 0x1d\nand then inspects the `RSA_public_decrypt`, `EVP_PKEY_set1_RSA`, and `RSA_get0_key` PLT shims. If at least one of them is\nresolved it returns TRUE so later code can jump through the host's libcrypto. When all three slots are still NULL it plants\n`backdoor_init_stage2` / `init_shared_globals` in the RSA entries as crash-safe fallbacks and returns FALSE so stage two keeps\nwaiting until the imports are ready.",
  "init_ldso_ctx": "Restores every ld.so flag the implant may have touched: it writes the saved auditstate bindflags back to libcrypto/sshd, unsets\nthe copied `l_name` byte, clears the `l_audit_any_plt` bit with the mask recovered earlier, and zeros `_dl_naudit`/`_dl_audit`\nso the dynamic linker no longer believes an audit module is registered. Stage two calls it on failure paths so sshd resumes with\nthe original ld.so state.",
  "init_shared_globals": "Seeds the shared global block with the mm/EVP hook entry points and a pointer to the lone `global_ctx` instance. Every hook\nconsults this block at runtime, so the function simply wires the exported function pointers into the struct and returns success\nonce the pointer checks pass.",
  "is_endbr64_instruction": {
    "plate": "Fast equality test used when scanning for CET landing pads. When at least four bytes remain it ORs ENDBR64 (`0xF30F1EFA`) with the caller-supplied `low_mask_part` so both ENDBR64 and ENDBR32 collapse into a single signature, then compares the resulting dword against the bytes at `code_start`. Returns TRUE only when the stream contains a full ENDBR instruction; otherwise the prologue walkers keep scanning.",
    "inline": [
      {
        "match": "if (3 < (long)* - (long)*) {",
        "match_type": "wildcard",
        "comment": "Require a full dword window before comparing against the masked ENDBR32/64 signature."
      }
    ]
  },
  "is_gnu_relro": {
    "plate": "Obfuscated equality test for PT_GNU_RELRO. Instead of comparing `p_type` directly against `0x6474e552`, the code adds the caller supplied `addend` (always `0xa0000000`) plus 1 and checks for the wrapped constant, which makes the instruction stream look less like a straightforward RELRO probe in the object file.",
    "inline": [
      {
        "match": "return (BOOL)(p_type + 1 + addend == 0x474e553);",
        "comment": "The arithmetic bakes the real magic (`0x6474e552`) into a wrapped constant so static scanners never see the literal."
      }
    ]
  },
  "is_range_mapped": {
    "plate": "User-space range probe that avoids `mincore(2)`. The helper rejects zero-length requests and addresses below 0x01000000, then requires `ctx->libc_imports` to expose `pselect` plus `__errno_location`. Starting from the page-aligned base it repeatedly points `pselect`'s `sigmask` argument at the address being tested while passing NULL fd sets; the kernel copies the `sigset_t`, so an unmapped byte causes `EFAULT`. The cursor advances in 0x200-byte steps until it covers `[addr, addr+length)`, clearing errno and returning FALSE on the first fault. Successful sweeps report TRUE so callers know the buffer is safe to dereference.",
    "inline": [
      {
        "match": "if (addr < (u8 *)0x1000000) {",
        "match_type": "wildcard",
        "comment": "Avoid probing NULL/vsyscall/etc.—the helpers never touch addresses below 16 MB."
      },
      {
        "match": "imports = ctx->libc_imports;",
        "match_type": "wildcard",
        "comment": "Every iteration insists that both `pselect` and `__errno_location` are exported before attempting the probe."
      },
      {
        "match": "pselect_result = (*libc_imports->pselect)(1,(fd_set *)0x0,(fd_set *)0x0,(fd_set *)0x0,(timespec *)&timeout_seconds",
        "match_type": "wildcard",
        "comment": "Abuse `pselect`'s signal-mask copy: the kernel will touch `probe_cursor`, which faults if the range is unmapped."
      },
      {
        "match": "if ((pselect_result < 0) &&",
        "match_type": "wildcard",
        "comment": "Treat `EFAULT` (or a NULL probe pointer) as \"unmapped\", clear errno, and bail out immediately."
      }
    ]
  },
  "j_tls_get_addr": {
    "plate": "Thin trampoline that jumps straight into glibc's `__tls_get_addr`. Stage two keeps both exports (the trapping stub and this wrapper) alive so relocations can point at the trap until the loader patches GOT entries to the legit resolver via `j_tls_get_addr`.",
    "inline": [
      {
        "match": "resolved_tls = __tls_get_addr(ti);",
        "comment": "Always delegate to glibc—the wrapper only exists so the hook infrastructure has a trusted target."
      }
    ]
  },
  "main_elf_parse": {
    "plate": "Parses the saved ld.so headers inside `main_elf_t`, resolves the versioned `__libc_stack_end` symbol, and confirms the captured runtime is sshd before publishing the pointer for later stages. Successful runs hand later hooks a stable way to reach argv/envp via `main_elf->__libc_stack_end`.",
    "inline": [
      {
        "match": "parse_ok = elf_parse(main_elf->dynamic_linker_ehdr,main_elf->elf_handles->ldso);",
        "comment": "Re-parse ld.so using the cached ELF header so the ldso `elf_info_t` is populated."
      },
      {
        "match": "libc_stack_end_sym = elf_symbol_get(main_elf->elf_handles->ldso,STR_libc_stack_end,STR_GLIBC_2_2_5)",
        "comment": "Resolve the versioned `__libc_stack_end` symbol from the interpreter image."
      },
      {
        "match": "libc_stack_end_ptr = elf->elfbase->e_ident + libc_stack_end_sym->st_value;",
        "comment": "Convert the symbol's st_value into a pointer inside ld.so's ELF image (double indirection)."
      },
      {
        "match": "parse_ok = process_is_sshd(elf,*(u8 **)libc_stack_end_ptr);",
        "comment": "Use `__libc_stack_end` to read sshd's argv/envp pointer and confirm the process really is sshd."
      },
      {
        "match": "*main_elf->__libc_stack_end = *(void **)libc_stack_end_ptr;",
        "comment": "Publish the resolved pointer so later hooks can reach sshd's stack without redoing the ELF walk."
      }
    ]
  },
  "mm_answer_authpassword_hook": {
    "plate": "Short-circuits `MONITOR_REQ_AUTHPASSWORD`: validates the ssh/monitor arguments, replays the payload-provided reply when\n`pending_authpayload_len/pending_authpayload` are set, or synthesizes the four-word success packet (request ID + root flag)\nwhen no payload chunk is queued. The reply is emitted via `fd_write()` and the saved dispatch entry is restored so sshd\ncontinues as if the original routine ran; malformed inputs fall back to libc’s `exit(0)` so the monitor never stays\nhalf-patched.",
    "inline": [
      {
        "match": "if ((m == (sshbuf *)0x0 || sock < 0) || (ssh == (ssh *)0x0)) {",
        "comment": "Missing ssh or monitor arguments mean the hook can’t safely forge a reply—exit immediately to avoid corrupting sshd’s dispatcher."
      },
      {
        "match": "if ((*(ushort *)(sshd_ctx + 0x90) == 0) ||",
        "comment": "When no payload queued an authpassword body, build the canned success packet locally using the cached request ID and PermitRootLogin tweak."
      },
      {
        "match": "fd_write(sock,reply_buf,reply_len,libc_imports);",
        "comment": "Whether the reply came from the payload or was synthesized on the stack, push it straight to the monitor socket so sshd never re-enters its password handler."
      },
      {
        "match": "**(undefined8 **)(sshd_ctx + 0xa0) = *(undefined8 *)(sshd_ctx + 0xd0);",
        "comment": "Restore the saved monitor dispatch entry so the next request drops back into sshd’s genuine `mm_answer_authpassword` implementation."
      }
    ]
  },
  "mm_answer_keyallowed_hook": {
    "plate": "Runs the decrypted payload state machine. It first validates `payload_state`, extracts sshbuf chunks from the monitor\nmessage, and when state==0 it copies the signed header into `payload_data`, decrypts it via `secret_data_get_decrypted`,\nand verifies the Ed448 signature against the cached host key. State 1 and 2 append additional chunks until the\nadvertised body_length is consumed, then state 3 interprets the decrypted command: copying payloads for\nmm_answer_keyverify/mm_answer_authpassword, invoking `sshd_proxy_elevate` to run system/PAM commands, or queueing auth\npayloads. On success it patches the monitor dispatch table to point at the attacker's hooks before tail-calling the\ngenuine `mm_answer_keyallowed`; any failure resets `payload_state` (or even exits sshd) so no partially decrypted data\nis reused.",
    "inline": [
      {
        "match": "state_ok = sshbuf_extract(m,ctx,(void **)&payload_ctx,(size_t *)&orig_handler);",
        "comment": "Pull the next sshbuf payload chunk straight out of the monitor message so the streaming decrypt can resume where it left off."
      },
      {
        "match": "decrypt_payload_message((key_payload_t *)payload_ctx,payload_chunk_size,ctx);",
        "comment": "Decrypt the framed chunk, append it into `ctx->payload_buffer`, and advance `payload_state` if enough bytes have arrived."
      },
      {
        "match": "if (payload_type == '\\x02') {",
        "comment": "Type 2 payloads carry a complete `mm_answer_keyverify` reply—copy its length/buffer into `sshd_ctx` and write it back immediately."
      },
      {
        "match": "else if (payload_type == '\\x03') {",
        "comment": "Type 3 payloads request privilege escalation: honor the supplied uid/gid pair and exec the decrypted body via libc’s `system()`."
      },
      {
        "match": "else if (((payload_type == '\\x01') &&",
        "comment": "Type 1 payloads stash an authpassword body for later—record the length/pointer so the authpassword hook can emit it on demand."
      },
      {
        "match": "state_ok = sshd_patch_variables(TRUE,FALSE,FALSE,0,ctx);",
        "comment": "As soon as an authpassword payload is queued, refresh PermitRootLogin/PAM/request IDs so the follow-on hook won’t trip sshd’s guards."
      },
      {
        "match": "state_ok = secret_data_get_decrypted(payload_seed_buf,ctx);",
        "comment": "State 0: decrypt the signed header seed into `payload_seed_buf` before copying the 0x3a-byte header into `ctx->payload_ctx`."
      },
      {
        "match": "ctx->payload_ctx->ed448_signature,payload_seed_buf,ctx),",
        "comment": "Verify the Ed448 signature over the fixed header before allowing the state machine to advance beyond stage zero."
      },
      {
        "match": "ctx->payload_ctx->signed_header_prefix,ctx);",
        "comment": "State 1 completion: once the final body chunk is spliced in, re-run the signature check across the assembled buffer before switching to command execution."
      }
    ]
  },
  "mm_answer_keyverify_hook": {
    "plate": "Short-circuits `MONITOR_REQ_KEYVERIFY` by streaming the payload-staged reply instead of running sshd’s verifier. Once\n`global_ctx` exposes libc imports and `sshd_ctx` recorded a reply length/buffer, the hook writes the blob to the monitor\nsocket, restores the saved dispatch slot, and reports success; missing metadata or a failed write triggers libc’s\n`exit(0)` so sshd never continues with a partially installed hook.",
    "inline": [
      {
        "match": "if ((*(ushort *)(sshd_ctx + 0x84) != 0) &&",
        "comment": "Only run when keyallowed already staged both the reply length and buffer; otherwise keep sshd’s original handler."
      },
      {
        "match": "(write_result = fd_write(sock,*(void **)(sshd_ctx + 0x88),(ulong)*(ushort *)(sshd_ctx + 0x84),libc_imports),",
        "comment": "Send the canned reply straight to the monitor socket so sshd believes the keyverify exchange already succeeded."
      },
      {
        "match": "**(undefined8 **)(sshd_ctx + 0xa0) = *(undefined8 *)(sshd_ctx + 0xd8);",
        "comment": "Drop the preserved mm_answer_keyverify pointer back into the live dispatch slot before returning success."
      },
      {
        "match": "if (libc_imports->exit != (pfn_exit_t)0x0) {",
        "comment": "Any missing metadata or short write forces an immediate `exit(0)` so sshd never continues with a half-applied hook."
      }
    ]
  },
  "mm_log_handler_hook": {
    "plate": "Interposes on sshd's log handler, bailing out entirely when logging has been globally disabled or sshd already dropped\nprivileges back to the sandbox. In filtering mode it scans the formatted string for the `\"Connection closed by ...\n(preauth)\"` pattern, rebuilds a safe replacement message from attacker-provided format strings, and emits it through\n`sshd_log()` while optionally muting syslog via the saved libc pointers. Messages that mention accepted authentication\nevents trigger a second rewrite path so only the sanitised strings ever reach sshd's real logger.",
    "inline": [
      {
        "match": "if (log_ctx_state->log_squelched == TRUE) {",
        "comment": "Respect the payload’s \"logging disabled\" flag—once set, every log request immediately bails."
      },
      {
        "match": "if ((string_id == STR_Accepted_password_for) || (string_id == STR_Accepted_publickey_for)) {",
        "comment": "Track log lines announcing a successful authentication so the hook can harvest the username/host fragments for rewriting."
      },
      {
        "match": "if (((log_ctx_state->syslog_mask_applied != FALSE) && (libc_imports != 0)) &&",
        "match_type": "wildcard",
        "comment": "Temporarily force `setlogmask(0xff)` whenever syslog suppression is enabled so sshd’s own handler stays quiet while we inject a sanitized line."
      },
      {
        "match": "if ((user_fragment_len != 0) && (host_fragment_len != 0)) {",
        "comment": "Only rebuild the \"Connection closed by … (preauth)\" string once both the username and host fragments were captured."
      },
      {
        "match": "(**(code **)(libc_imports + 0x58))(0x80000000);",
        "comment": "Restore sshd’s original syslog mask after the sanitized message has been emitted."
      }
    ]
  },
  "process_is_sshd": {
    "plate": "Walks the argc/argv/envp layout straight off the caller’s stack pointer and only returns TRUE when the process really looks like sshd. It demands a sane argc, an argv[0] pointer that lives on the stack and hashes to `/usr/sbin/sshd`, and then walks argv[1…] ensuring every pointer stays within 0x4000 bytes of the saved SP and never triggers `check_argument`’s debug filter. Once argv terminates, envp entries must either remain stack-resident or fall inside sshd’s writable `.data/.bss` span, and any environment string that maps to a known encoded ID aborts the probe.",
    "inline": [
      {
        "match": "EVar3 = get_string_id((char *)argv_entry,(char *)0x0);",
        "comment": "Hash argv[0] and insist it decodes to `/usr/sbin/sshd` before scanning any further."
      },
      {
        "match": "while (more_args_to_scan = arg_index != argc, arg_index = arg_index + 1, more_args_to_scan) {",
        "comment": "Iterate over argv[1..argc-1], checking each pointer range before handing it to the debug filter."
      },
      {
        "match": "if ((argv_entry <= stack_end) || (0x4000 < (ulong)((long)argv_entry - (long)stack_end))) {",
        "comment": "Stack-relative argv/env pointers must land within 0x4000 bytes of the saved SP; anything else falls through to the `.data` guard."
      },
      {
        "match": "debug_match = check_argument\\(",
        "match_type": "regex",
        "comment": "Let the helper spot strings containing lowercase `d` so sshd’s debug modes never reach the hooks."
      },
      {
        "match": "if (*(long *)(stack_end + arg_index * 8) == 0) {",
        "comment": "Switch to envp processing only after confirming argv was NULL-terminated."
      },
      {
        "match": "data_segment_base = \\(u8 \\*\\)elf_get_data_segment",
        "match_type": "regex",
        "comment": "When env pointers leave the stack, demand that they reside inside sshd’s writable `.data/.bss` range."
      },
      {
        "match": "EVar3 = get_string_id\\(\\(char \\*\\)\\*env_cursor,\\(char \\*\\)0x0\\);",
        "match_type": "regex",
        "comment": "Known environment keys (non-zero encoded IDs) are treated as hostile and abort the probe immediately."
      }
    ]
  },
  "process_shared_libraries": "Wrapper around `process_shared_libraries_map` that first resolves `r_debug` out of ld.so, copies the caller-provided struct into\na local scratch copy, and feeds the scratch copy into the map-walker. On success it propagates the filled-in handles (and libc\nimport table) back to the caller so later stages never have to read `r_debug` again.",
  "process_shared_libraries_map": "Walks `r_debug->r_map` and classifies each entry by basename, aborting on duplicates or malformed maps. Only after locating sshd\n(the main binary), libcrypto, ld-linux, libsystemd, liblzma, and libc does it parse the ELF images: sshd’s PLT is interrogated\nto recover `RSA_public_decrypt`, `EVP_PKEY_set1_RSA`, and `RSA_get0_key`, liblzma’s RW data segment is recorded so the\n`backdoor_hooks_data_t` blob and `hooks_data_addr` can be cached, libcrypto/libc descriptors are primed for later walkers, and\nlibc’s import table is filled via `resolve_libc_imports`. The result is a fully-populated `backdoor_shared_libraries_data_t` for\ndownstream stages.",
  "resolve_libc_imports": "Treats `link_map *libc` as another ELF image, runs `elf_parse` to populate `elf_info_t`, and then allocates trampolines for\n`read` and `__errno_location` via the fake allocator shim. Only when both imports succeed does it mark `libc_imports_t` as\nready, ensuring subsequent socket I/O helpers can operate without touching the real PLT.",
  "rsa_key_hash": "Grabs the exponent and modulus via RSA_get0_key, serialises the exponent first and the modulus second with bignum_serialize into\na ~4 KiB stack buffer, and runs sha256 over the exact number of bytes produced. Any missing component or overflow of the\n0x100a-byte scratch cancels the fingerprint.",
  "run_backdoor_commands": "Master dispatcher for the RSA hooks. It refuses to run unless the secret-data bitmap is complete, extracts the modulus and\nexponent via RSA_get0_key, and uses the modulus bytes as a transport for an encrypted payload header/body. The body is decrypted\nwith the ChaCha keys from secret_data_get_decrypted, every cached sshd host key is hashed (rsa_key_hash/dsa_key_hash/etc.) until\nthe embedded Ed448 signature verifies, and the resulting command toggles global_ctx state (sshd_offsets, syslog/PAM controls,\nsocket selection, payload streaming state). When a payload wants execution it populates a monitor_data_t and calls\nsshd_proxy_elevate; otherwise it patches sshd variables/logging in place. Any parse/signature failure sets\nctx->disable_backdoor, leaves *do_orig = TRUE, and the real OpenSSL routine proceeds untouched.",
  "secret_data_append_from_address": "Lets hooks fingerprint themselves without a static code pointer. If addr is NULL/1 it substitutes the caller’s return address,\notherwise it uses the explicit address, and in either case it forwards both the call site and the resolved code pointer to\nsecret_data_append_singleton.",
  "secret_data_append_from_call_site": "Shortcut used directly inside hooks: it grabs the caller’s return address (unaff_retaddr), runs secret_data_append_singleton\nwith it, and ORs the result with the supplied bypass flag so instrumentation sites can opt out when the attestation fails.",
  "secret_data_append_from_code": "Sweeps a code range and feeds instructions to secret_data_append_from_instruction. When start_from_call is TRUE it first finds\nthe next CALL via find_call_instruction, then loops up to shift_count times, each time calling find_reg2reg_instruction to\nlocate a qualifying instruction and shifting the supplied cursor. Returning FALSE means it could not find enough instructions in\nthe provided span.",
  "secret_data_append_from_instruction": "Sets the next bit inside global_ctx->secret_data based on a decoded instruction. The cursor enforces the 0x1C8-bit ceiling,\nskips certain opcodes (0x109, 0xBB, and entries in the precomputed 0x83–0xB1 mask), and otherwise locates the byte/bit inside\nthe secret_data array and ORs it in before advancing the cursor.",
  "secret_data_append_item": "Convenience wrapper used by the secret-data descriptor tables: when the supplied index is non-zero it simply calls\nsecret_data_append_singleton with the provided code pointer and cursor, otherwise it treats the entry as disabled and reports\nFALSE so the batch runner can bail out early.",
  "secret_data_append_items": "Batch driver for arrays of secret_data_item_t. It assigns sequential indexes to entries that have not been initialised yet,\ncalls the provided appender for each descriptor (passing the 1-based ordinal and the recorded code pointer), and stops at the\nfirst failure so callers know whether the entire batch completed.",
  "secret_data_append_singleton": "Guarantees each attestation slot runs at most once. It uses the per-operation byte array inside global_ctx->shift_operations to\nguard entry, resolves the function boundaries with find_function relative to the recorded sshd code limits, invokes\nsecret_data_append_from_code (starting after the call site if present), and increments global_ctx->num_shifted_bits by\nshift_count on success.",
  "secret_data_get_decrypted": "Unwraps the 57-byte global_ctx->secret_data blob with two ChaCha passes. A baked-in key/IV pair (the key_buf constants) decrypts\na 0x30-byte seed, that seed becomes the real ChaCha key, and a second decrypt peels the runtime secret into the caller buffer\nusing another static IV. All operations go through the resolved EVP entry points so no static crypto ships with the implant.",
  "sha256": "Thin wrapper around EVP_Digest/Evp_sha256: it rejects empty buffers, refuses to write unless mdBuf has at least 32 bytes of\nspace, looks up OpenSSL’s SHA-256 implementation via the import table, and hashes the supplied payload in place.",
  "sshbuf_bignum_is_negative": "Treats an sshbuf as a serialized big integer. When the payload is between 0x20 and 0x40 bytes it scans forward until it finds a\nbyte with the sign bit set; encountering such a byte before hitting the end marks the buffer as “negative” and therefore\nsuitable for modulus harvesting.",
  "sshbuf_extract": "Reads out an sshbuf’s d pointer and size field using the dynamic layout encoded in global_ctx->sshd_offsets. Depending on\nwhether each qword index is negative it either uses the struct fields directly or walks to the encoded offset, confirms both the\nstruct and the pointed-to range are mapped with is_range_mapped, and hands the caller the live pointer/length pair.",
  "sshd_configure_log_hook": {
    "plate": "Validates that the caller provided writable log handler slots plus the format strings needed to rewrite messages, and\nonly honours logging requests when the controlling flag (bit 3 in `cmd_flags->flags1`) is set or the backdoor is already\nrunning as root. If the existing handler/context pointers already reside inside sshd it swaps them so the implant can\nhijack them safely, snapshots the original function/context, and either disables logging entirely or enables filtering\nmode. In filter mode it verifies that the `%s`, `\"Connection closed by\"`, and `\"(preauth)\"` strings are available before\ndropping `mm_log_handler_hook` into place.",
    "inline": [
      {
        "match": "if (((((cmd_flags == (cmd_arguments_t *)0x0) || (log_ctx == (sshd_log_ctx_t *)0x0)) ||",
        "match_type": "wildcard",
        "comment": "Bail unless the log context, handler slot, ctx slot, and hook entry were all recovered."
      },
      {
        "match": "if ((log_flag == 0) || (ctx->caller_uid == 0)) {",
        "comment": "Only rewire logging when the control bit requested it or the implant is already running as root."
      },
      {
        "match": "if ((saved_ctx_value != (log_handler_fn)0x0) &&",
        "match_type": "wildcard",
        "comment": "Swap the handler/context slots when the saved pointer already lives inside sshd so patching stays safe."
      },
      {
        "match": "if (log_ctx->fmt_percent_s == (char *)0x0) {",
        "comment": "Filter mode requires the `%s`, `Connection closed by`, and `(preauth)` strings; missing any of them aborts."
      },
      {
        "match": "*log_handler_slot = (log_handler_fn)log_ctx->log_hook_entry;",
        "comment": "Whichever slot currently holds the log handler pointer is overwritten with `mm_log_handler_hook`."
      }
    ]
  },

  "sshd_find_main": {
    "plate": "Walks sshd's entry thunk from `Elf64_Ehdr::e_entry`, bounding the decoder to the first 0x200 bytes of `.text` so it only\nhas to understand the glibc crt1 shim. The helper temporarily points the fake lzma allocator at libcrypto, resolves all required\nEVP helpers up front, and then looks for a RIP-relative MOV/LEA that produces an address inside sshd's text segment. The very\nnext CALL must target `__libc_start_main@GOT` through the same register, at which point the discovered `sshd_main` pointer and\nfully primed `imported_funcs` table are returned to the caller.",
    "inline": [
      {
        "match": "allocator->opaque = libcrypto;",
        "comment": "Point the fake allocator at libcrypto so the `lzma_alloc` shim can resolve EVP helpers from that module."
      },
      {
        "match": "code_end = code_start + 0x200;",
        "comment": "Only scan the crt1-sized entry stub—clamp the walk to 0x200 bytes or the end of `.text`, whichever comes first."
      },
      {
        "match": "symbol_entry = elf_symbol_get(libcrypto,STR_EVP_Digest,0);",
        "comment": "Preload EVP_Digest before decoding so the import table is ready as soon as the entry point is confirmed."
      },
      {
        "match": "if (*(u32 *)&insn_ctx.opcode_window[3] == 0x10d) {",
        "comment": "Treat RIP-relative MOV/LEA instructions that resolve inside sshd's code segment as the prospective `sshd_main` pointer."
      },
      {
        "match": "else if (((sshd_main_candidate != (u8 *)0x0) && (*(u32 *)&insn_ctx.opcode_window[3] == 0x17f)) &&",
        "comment": "The capture is only valid when the very next CALL targets `__libc_start_main@GOT` via the same register."
      }
    ]
  },
  "sshd_find_monitor_field_addr_in_function": {
    "plate": "Disassembles a monitor helper, finds a MOV/LEA that pulls from sshd's `.data/.bss`, and then spends the next ~0x40 bytes tracking that register through copies until it lands in RDI and flows into `mm_request_send`. When every predicate fires the referenced `.bss` address is returned as the monitor struct field (sendfd/recvfd/sshbuf pointer, etc.).",
    "inline": [
      {
        "match": "decode_ok = find_mov_lea_instruction(code_start,code_end,TRUE,TRUE,&insn_ctx);",
        "comment": "Seed the analysis by re-running the MOV/LEA scanner until a writable sshd address is loaded into a register."
      },
      {
        "match": "if ((data_start <= monitor_field_addr) && (monitor_field_addr < data_end)) {",
        "comment": "Only consider MOV/LEA hits that touch sshd's `.data/.bss` window—the monitor struct lives there."
      },
      {
        "match": "call_window_end = code_start + 0x40;",
        "comment": "Clamp the search window to roughly 0x40 bytes so only the prologue-sized snippet is analysed."
      },
      {
        "match": "if (mirrored_reg == 7) {",
        "comment": "Once the tracked pointer flows into RDI (argument register 7) the helper expects a nearby `mm_request_send` call."
      },
      {
        "match": "decode_ok = find_call_instruction",
        "comment": "Verify that the mm_request_send call immediately follows; if it does, the captured address becomes the monitor slot."
      }
    ]
  },
  "sshd_find_monitor_struct": {
    "plate": "Instruments the ten monitor-side helpers referenced in `string_refs` (allocation, channel handling, recv/send paths, etc.) by calling `sshd_find_monitor_field_addr_in_function` for each one. Every returned BSS address is tallied, and once a value shows up at least five times the routine records it in `ctx->struct_monitor_ptr_address` so later hooks can dereference monitor->monitor_to_child_fd/child_to_monitor_fd directly. The helper also emits a `secret_data_append_from_call_site` breadcrumb so the secret-data mirroring code knows when monitor discovery succeeded.",
    "inline": [
      {
        "match": "secret_append_ok = secret_data_append_from_call_site((secret_data_shift_cursor_t)0xda,0x14,0xf,FALSE);",
        "comment": "Log the monitor discovery entry point so the secret-data tap can mirror that progress later."
      },
      {
        "match": "ctx->sshd_ctx->mm_request_send_start != (void *)0x0",
        "comment": "Abort when the mm_request metadata is missing—without it there is no stable monitor struct to discover."
      },
      {
        "match": "data_start = (u8 *)elf_get_data_segment(elf,&data_segment_size,FALSE);",
        "comment": "Limit all candidate addresses to sshd’s writable data segment so stray pointers never skew the vote."
      },
      {
        "match": "monitor_vote_table[0] = 4;",
        "comment": "Seed the first half of the vote table with the string-reference indexes for the ten monitor helper functions we trust."
      },
      {
        "match": "for (vote_inner_idx = 0x14; vote_inner_idx != 0; vote_inner_idx = vote_inner_idx + -1) {",
        "comment": "Zero both the candidate slots and the tail half of the vote table before collecting fresh samples."
      },
      {
        "match": "code_start = (u8 *)(&refs->xcalloc_zero_size)[monitor_vote_table[vote_idx]].func_start;",
        "comment": "Walk each monitor helper via its cached function bounds before searching for BSS writes."
      },
      {
        "match": "          sshd_find_monitor_field_addr_in_function",
        "comment": "Ask the helper to look for MOV [mem],reg stores and cache whatever monitor struct pointer those routines touch.",
        "placement": "after"
      },
      {
        "match": "vote_cursor = monitor_vote_table + 10;",
        "comment": "Reuse the upper ten entries of the vote table as counters that track how many times each slot matched."
      },
      {
        "match": "if ((uint)vote_inner_idx <= (uint)candidate_slot) {",
        "comment": "Empty candidate buckets fall through to here, causing the next unused slot to inherit the vote."
      },
      {
        "match": "monitor_vote_table[winning_candidate_idx + 10] = monitor_vote_table[winning_candidate_idx + 10] + 1;",
        "comment": "Otherwise increment the counter for whichever pointer matched so the most popular candidate can be selected later."
      },
      {
        "match": "if ((4 < top_vote_count) && ((monitor **)monitor_candidates[winning_candidate_idx] != (monitor **)0x0)) {",
        "comment": "Only accept a result once at least five helpers agreed on the same pointer, which filters out incidental hits."
      }
    ]
  },
  "sshd_find_sensitive_data": {
    "plate": "Bootstraps the entire sensitive-data pipeline: it appends bookkeeping entries for `sshd_proxy_elevate`/socket helpers into the secret-data log, uses the fake lzma allocator (pointed at libcrypto) to resolve `EVP_PKEY_new_raw_public_key`, `EVP_Digest`, `EVP_DigestVerify`, `EVP_DigestVerifyInit`, `EVP_CIPHER_CTX_new`, `EVP_chacha20`, and sanity-checks that the library exports the `EVP_sm*` family the payload expects. It locates sshd's code/data segments, finds the real `sshd_main` entry (recording whether an ENDBR64 prefix is present), and runs both the xcalloc-based and `KRB5CCNAME` heuristics to recover candidate struct addresses. Each candidate is scored via `sshd_get_sensitive_data_score`, and whichever pointer clears the >=8 threshold is stored in `ctx->sshd_sensitive_data`; on failure all of the just-resolved libcrypto stubs are freed before the helper reports that no recon data was found.",
    "inline": [
      {
        "match": "operation_ok = secret_data_append_from_address((void *)0x0,(secret_data_shift_cursor_t)0x1c8,0,0x1d);",
        "comment": "Emit breadcrumbs for the monitor command handlers so later refreshes can see that the recon code ran."
      },
      {
        "match": "secret_probe_items.anchor_pc = (u8 *)sshd_proxy_elevate;",
        "comment": "Pre-populate a four-entry batch that records the proxy elevate helper plus the monitor socket discovery routines."
      },
      {
        "match": "operation_ok = secret_data_append_items(&secret_probe_items,4,secret_data_append_item);",
        "comment": "Push all four breadcrumbs into the log in one shot; a failure here aborts before any ELF parsing happens."
      },
      {
        "match": "allocator = get_lzma_allocator();",
        "comment": "Point the fake lzma allocator at libcrypto so subsequent `lzma_alloc` calls actually resolve EVP helpers."
      },
      {
        "match": "allocator->opaque = libcrypto;",
        "comment": "Remember the target image so the allocator resolves symbols inside libcrypto instead of sshd."
      },
      {
        "match": "digest_verify_init = (pfn_EVP_DigestVerifyInit_t)lzma_alloc(0x118,allocator);",
        "comment": "Resolve and pin each crypto helper by allocating a stub from libcrypto; success bumps `resolved_imports_count`."
      },
      {
        "match": "text_segment = elf_get_code_segment(sshd,&code_segment_size);",
        "comment": "Grab sshd’s text segment (and later the data segment) so both heuristics operate on real in-memory bounds."
      },
      {
        "match": "digest_verify_sym = elf_symbol_get(libcrypto,STR_EVP_DigestVerify,0);",
        "comment": "Check that libcrypto export tables still contain the EVP entry points the payload expects to hijack."
      },
      {
        "match": "operation_ok = sshd_find_main(&sshd_main_addr,sshd,libcrypto,funcs);",
        "comment": "Locate the actual `main()` body and remember its address for later scoring and hook decisions."
      },
      {
        "match": "operation_ok = is_endbr64_instruction(sshd_main_addr,sshd_main_addr + 4,0xe230);",
        "comment": "Capture whether sshd used CET/ENDBR64 so downstream patches can keep the landing pad intact."
      },
      {
        "match": "operation_ok = sshd_get_sensitive_data_address_via_xcalloc",
        "comment": "Use the xcalloc heuristic to find a struct candidate by following the xcalloc(result) stores into .bss."
      },
      {
        "match": "krb_candidate_found = sshd_get_sensitive_data_address_via_krb5ccname",
        "comment": "Run the independent KRB5CCNAME-based scan in parallel so two separate heuristics can vote on the same address."
      },
      {
        "match": "xzcalloc_score = sshd_get_sensitive_data_score(xzcalloc_candidate_local,sshd,refs);",
        "comment": "Score whichever candidate(s) were recovered; the function only accepts pointers that reach eight or more points."
      },
      {
        "match": "ctx->sshd_sensitive_data = winning_candidate;",
        "comment": "Persist the winning pointer into the global context so every hook can dereference sshd’s sensitive_data struct."
      },
      {
        "match": "lzma_free(funcs->EVP_DigestVerifyInit,allocator);",
        "comment": "Tear down any temporary EVP handles when discovery fails so the loader does not leak libcrypto objects."
      }
    ]
  },
  "sshd_get_client_socket": {
    "plate": "Prefers sshd’s monitor struct when it has already been located: the helper validates that the pointer is still mapped, selects `child_to_monitor_fd` or `monitor_to_child_fd` based on the requested direction, and probes the descriptor with a zero-length `read()` (retrying on EINTR but rejecting EBADF). If the monitor is missing or the fd is dead it falls back to `sshd_get_usable_socket` and hands back the Nth idle descriptor instead.",
    "inline": [
      {
        "match": "monitor_candidate = *ctx->monitor_struct_slot;",
        "comment": "Use the recovered monitor struct when one was published through `global_context_t`."
      },
      {
        "match": "monitor_mapped = is_range_mapped\\(",
        "match_type": "regex",
        "comment": "Skip the monitor path entirely if the cached pointer is unmapped or stale."
      },
      {
        "match": "if (socket_direction == DIR_WRITE) {",
        "comment": "DIR_WRITE expects the child→monitor pipe; DIR_READ grabs the monitor→child side."
      },
      {
        "match": "read_result = \\(\\*libc_imports->read\\)",
        "match_type": "regex",
        "comment": "Issue a zero-length read to confirm the fd is alive, retrying on EINTR but treating EBADF as fatal."
      },
      {
        "match": "monitor_mapped = sshd_get_usable_socket\\(",
        "match_type": "regex",
        "comment": "Fall back to the brute-force fd scanner when the monitor probe failed."
      }
    ]
  },
  "sshd_get_sensitive_data_address_via_krb5ccname": {
    "plate": "Starts from the unique `\"KRB5CCNAME\"` reference, proves that getenv's return value is copied into sshd's `.data/.bss` region with the familiar -0x18 stride, and hands the caller the computed struct base. It tolerates both register-tracking MOV sequences and the LEA/zero-immediate variant OpenSSH uses when the pointer is materialised directly.",
    "inline": [
      {
        "match": "krb5_string_ref = elf_find_string_reference(elf,STR_KRB5CCNAME,code_start,code_end);",
        "comment": "Use the cached string table to jump straight to the block that references `KRB5CCNAME`."
      },
      {
        "match": "store_scan_cursor = string_scan_ctx.instruction + string_scan_ctx.instruction_size;",
        "comment": "After spotting the getenv result, walk the next few instructions looking for stores into `.bss`."
      },
      {
        "match": "candidate_store = (u8 *)0x0;",
        "comment": "The following RIP-relative add reconstructs the absolute `.bss` pointer that getenv's return register is being stored into."
      },
      {
        "match": "data_cursor = candidate_store + -0x18;",
        "comment": "Back up by 0x18 bytes to convert the field pointer into the `sensitive_data` base address."
      },
      {
        "match": "else if (*(u32 *)&string_scan_ctx.opcode_window[3] == 0x147) {",
        "comment": "Fallback for the LEA/zero-immediate pattern that writes the struct pointer without first capturing getenv's return register."
      }
    ]
  },
  "sshd_get_sensitive_data_address_via_xcalloc": {
    "plate": "Consults the cached string references to find sshd's zero-initialisation `xcalloc` call, watches the next handful of instructions for `.bss` stores of the return value, and records up to sixteen unique destinations. Whenever it sees three pointers separated by eight bytes (ptr/ptr+8/ptr+0x10) it treats the lowest slot as the `sensitive_data` base.",
    "inline": [
      {
        "match": "xcalloc_call_target = (u8 *)(string_refs->xcalloc_zero_size).func_start;",
        "comment": "Use the precomputed string catalogue to seed the scan with the `xcalloc` call site."
      },
      {
        "match_regex": "find_call_instruction\\(code_start,code_end,xcalloc_call_target,&store_probe_ctx\\)",
        "comment": "Hunt for the direct CALL into `xcalloc`; each hit restarts the post-call analysis window."
      },
      {
        "match": "decode_ok = find_instruction_with_mem_operand_ex",
        "comment": "Immediately scan the following bytes for a MOV [mem],reg instruction that stores the allocated pointer."
      },
      {
        "match": "store_operand_ptr = (u8 *)(store_probe_ctx.mem_disp + (long)store_probe_ctx.instruction) + store_probe_ctx.instruction_size;",
        "comment": "Convert the RIP-relative store into an absolute `.bss` pointer before recording it."
      },
      {
        "match": "if (((void *)store_hits[clear_idx] == (void *)(store_hits[hit_scan_idx] + -8)) &&",
        "comment": "Look for three slots spaced eight bytes apart; that stride matches the struct layout (base, base+8, base+0x10)."
      }
    ]
  },
  "sshd_get_sensitive_data_score": {
    "plate": "Combines the per-function heuristics by doubling the `demote_sensitive_data` and `main()` scores, adding them together, and finally tacking on the `do_child` result. Only candidates that reach eight or more points are surfaced to the rest of the implant; weaker hits are ignored even if one heuristic thought they were promising.",
    "inline": [
      {
        "match": "score_demote = sshd_get_sensitive_data_score_in_demote_sensitive_data(sensitive_data,elf,refs);",
        "comment": "Pull the high-confidence score from `demote_sensitive_data` first — those three points get doubled later."
      },
      {
        "match": "score_main = sshd_get_sensitive_data_score_in_main(sensitive_data,elf,refs);",
        "comment": "Fold in the `main()`-specific heuristic so candidates the daemon manipulates frequently accrue bonus weight."
      },
      {
        "match": "score_do_child = sshd_get_sensitive_data_score_in_do_child(sensitive_data,elf,refs);",
        "comment": "Finally, add any points earned inside `do_child`, which is the only part of the aggregate that is not doubled."
      },
      {
        "match": "return score_do_child + (score_demote + score_main) * 2;",
        "comment": "Demote/Main scores are doubled before adding the `do_child` total, yielding the >=8 threshold enforced by callers."
      }
    ]
  },
  "sshd_get_sensitive_data_score_in_demote_sensitive_data": {
    "plate": "Disassembles the string-identified `demote_sensitive_data` helper and returns three points as soon as any memory operand touches the candidate pointer. That helper is tightly coupled to the real struct, so even a single hit is treated as strong evidence in the aggregate score.",
    "inline": [
      {
        "match": "code_start = (u8 *)(refs->demote_sensitive_data).func_start;",
        "comment": "Leverage the cached string reference so we only run the scan when `demote_sensitive_data` was actually located."
      },
      {
        "match": "demote_hit = find_instruction_with_mem_operand",
        "comment": "Walk the routine until a MOV/LEA mentions the candidate struct pointer; that hit is worth the full three points."
      },
      {
        "match": "score = 3;",
        "comment": "Return an immediate +3 because any true reference inside `demote_sensitive_data` is a very strong signal."
      }
    ]
  },
  "sshd_get_sensitive_data_score_in_do_child": {
    "plate": "Uses the cached string reference for `do_child`, awards one point if it ever touches the candidate pointer, and then probes for one or two references to offset +0x10. The second half of the struct buys up to two additional points, yielding a 0–3 score that feeds the aggregate heuristic.",
    "inline": [
      {
        "match": "code_start = (u8 *)(refs->chdir_home_error).func_start;",
        "comment": "Locate `do_child` via the `chdir_home_error` string reference and bail out if the symbol is missing."
      },
      {
        "match": "hit_found = find_instruction_with_mem_operand(code_start,code_end,(dasm_ctx_t *)0x0,sensitive_data);",
        "comment": "Touching the base pointer once awards the initial point in the score."
      },
      {
        "match": "hit_found = find_instruction_with_mem_operand",
        "comment": "Reuse the same scanner to hunt for accesses to `sensitive_data + 0x10`; the first hit collects +1."
      },
      {
        "match": "hit_found = find_instruction_with_mem_operand",
        "occurrence": 3,
        "comment": "A second access to the +0x10 field within the remaining code bumps the score by another point."
      }
    ]
  },
  "sshd_get_sensitive_data_score_in_main": {
    "plate": "Looks inside the cached `main()` range and searches for memory references to the struct at offsets 0, +8, and +0x10. It gives +1 when the base is accessed, +1 when +0x10 is touched, and subtracts one when +8 never shows up, producing a signed score between -1 and +3. The result is doubled later so the decision logic favours pointers that the main daemon manipulates frequently.",
    "inline": [
      {
        "match": "code_start = (u8 *)(refs->list_hostkey_types).func_start;",
        "comment": "Reuse the `list_hostkey_types` string reference to bound sshd’s `main()` implementation before scanning."
      },
      {
        "match": "base_hit = find_instruction_with_mem_operand(code_start,code_end,(dasm_ctx_t *)0x0,sensitive_data);",
        "comment": "Award the first point when any instruction in `main()` touches the candidate struct base."
      },
      {
        "match": "offset10_hit = find_instruction_with_mem_operand",
        "comment": "Look for a second access at offset +0x10; most true positives tick that bookkeeping field at least once."
      },
      {
        "match": "offset8_hit = find_instruction_with_mem_operand",
        "comment": "Track whether the +8 slot ever gets referenced so we can penalise pointers that never resemble the real layout."
      },
      {
        "match": "score = (((uint)(base_hit != FALSE) - (uint)(offset10_hit == FALSE)) + 2) - (uint)(offset8_hit == FALSE);",
        "comment": "Collapse the three booleans into the signed -1…+3 score that later feeds the aggregate heuristic."
      }
    ]
  },
  "sshd_get_sshbuf": "Finds the sshbuf inside sshd’s monitor structure that now holds the forged modulus. It dereferences\nglobal_ctx->struct_monitor_ptr_address, uses the packed sshd_offsets to identify the m_pkex pointer and the sshbuf data/size\nfields, and validates any candidate via sshbuf_extract. When offsets are unknown it brute-forces the pkex table: two buffers\nmust decode to “SSH-2.0”/“ssh-2.0” string IDs and the next buffer must look like a negative bignum (sshbuf_bignum_is_negative).\nOnly then does it return the mapped sshbuf->d pointer and length.",
  "sshd_get_usable_socket": {
    "plate": "Brute-force walks file descriptors 0–63, calling `shutdown(fd, 0x7fffffff)` on each one and interpreting the expected `EINVAL`/`ENOTCONN` errors as proof the descriptor is open (but idle). Every qualifying fd increments a counter, and once it reaches `socket_index` the helper returns that fd so callers can recycle sshd’s sockets even if the monitor struct was never recovered.",
    "inline": [
      {
        "match": "shutdown_status = \\(\\*imports->shutdown\\)\\(sockfd,0x7fffffff\\);",
        "match_type": "regex",
        "comment": "Probe each descriptor with an invalid `how` value so active sockets report `EINVAL`/`ENOTCONN`."
      },
      {
        "match": "errno_ptr = \\(\\*imports->__errno_location\\)\\(\\);",
        "match_type": "regex",
        "comment": "Sample errno after failures so we can distinguish usable sockets from closed descriptors."
      },
      {
        "match": "if ((*errno_ptr != 0x16) && (*errno_ptr != 0x6b)) goto",
        "match_type": "wildcard",
        "comment": "Only descriptors that raise EINVAL or ENOTCONN count as \"usable\"."
      },
      {
        "match": "if (matches_seen == socket_index) {",
        "comment": "Return the fd once we’ve reached the requested ordinal."
      }
    ]
  },
  "sshd_log": {
    "plate": "Mirrors sshd’s `sshlogv()` calling convention. It saves the incoming SSE argument registers when the ABI says variadic vector arguments are present, rebuilds a fresh `va_list` (gp/fp offsets plus overflow/stack areas), and finally tail-calls the resolved `sshlogv` pointer stored in the logging context so higher-level hooks can format log lines exactly the way sshd expects.",
    "inline": [
      {
        "match": "if \\(in_AL != '\\\\0'\\)",
        "match_type": "regex",
        "comment": "When the caller flagged vector arguments, spill the incoming XMM registers so they can be replayed."
      },
      {
        "match": "va_list_state = &stack0x00000008;",
        "comment": "Recreate the gp/fp offsets, overflow area, and `va_list` pointer exactly the way sshlogv expects."
      },
      {
        "match": "\\(\\*\\(code \\*\\)log_ctx->sshlogv_impl\\)",
        "match_type": "regex",
        "comment": "Tail-call sshd’s real sshlogv() implementation so our wrapper stays transparent."
      }
    ]
  },
  "sshd_patch_variables": {
    "plate": "Requires the mm_answer_authpassword hook and metadata to have been recovered, then applies three optional tweaks: force\nPermitRootLogin to the value `3` (\"yes\"), zero out `use_pam` when PAM should be disabled, and replace sshd's monitor\ndispatch table entry with the attacker's authpassword hook. When `monitor_reqtype` isn't explicitly supplied it is\nderived from the original dispatch table so the forged replies stay in lock-step with sshd's state machine.",
    "inline": [
      {
        "match": "if ((((global_ctx == (global_context_t *)0x0) ||",
        "match_type": "wildcard",
        "comment": "Refuse to run until the global ctx, sshd_ctx, and mm_answer_authpassword hook are all populated."
      },
      {
        "match": "if (skip_root_patch == FALSE) {",
        "comment": "Clamp PermitRootLogin to 3 (\"yes\") whenever the caller didn't explicitly skip the root tweak."
      },
      {
        "match": "if (disable_pam != FALSE) {",
        "comment": "Zero `use_pam` only when sshd exposed a writable pointer and the payload asked for the PAM bypass."
      },
      {
        "match": "if (replace_monitor_reqtype == FALSE) {",
        "comment": "Derive the request ID from sshd's live dispatch table so forged replies stay in sync with the monitor state machine."
      },
      {
        "match": "*sshd_ctx->mm_answer_authpassword_slot = authpassword_hook;",
        "comment": "Finally drop the attacker's hook into the genuine slot once every optional tweak is satisfied."
      }
    ]
  },

  "sshd_proxy_elevate": {
    "plate": "Implements the privileged side of the monitor command channel. It validates the supplied RSA components, command flags,\nand libc/import tables; rejects unsupported command types; and decides whether PAM should be disabled or sshd should\nexit outright based on the flag bits. For KEYALLOWED-style payloads it hunts for the staged ChaCha-wrapped blob on the\nstack, decrypts and hashes it, emits a forged `MONITOR_REQ_KEYALLOWED` packet (creating temporary BIGNUM/RSA objects\npopulated with the attacker's modulus/exponent), and writes it over the selected monitor socket or the fd discovered via\n`sshd_get_client_socket`. After sending optional sshbuf payloads it drains replies, honours \"wait\" vs \"fire-and-forget\"\nsemantics, and updates the monitor request IDs so sshd's dispatcher believes the forged exchange was legitimate.",
    "inline": [
      {
        "match": "if (sshd_ctx->have_mm_answer_keyallowed == FALSE) {",
        "comment": "Before the mm hooks land, only the minimal control-plane commands are accepted; anything needing KEYALLOWED is rejected."
      },
      {
        "match": "if ((args->cmd_type == 3) && (monitor_flag_mask = cmd_args->monitor_flags & 0xc0, monitor_flag_mask != 0xc0)) {",
        "comment": "PRIV/EXIT payloads hunt the stack for the staged ChaCha blob, verify its hash, and decrypt it in place before forging the monitor request."
      },
      {
        "match": "success = bignum_serialize(monitor_req_payload + payload_remaining,payload_room,&serialized_chunk_len,rsa_components[digest_idx],imports);",
        "comment": "Serialise the attacker's exponent and modulus into the monitor frame so sshd sees a well-formed RSA keypair."
      },
      {
        "match": "status = (*ctx->imported_funcs->RSA_set0_key)(rsa_ctx,rsa_n_bn,rsa_e_bn,rsa_d_bn);",
        "comment": "Build a temporary RSA object from the supplied components in order to hash and sign the forged packet."
      },
      {
        "match": "if ((cmd_args->control_flags & 0x20) == 0) {",
        "comment": "Without an explicit socket override, call `sshd_get_client_socket`; otherwise honour the encoded socket selector bits."
      },
      {
        "match": "if ((cmd_type == 0) || ((cmd_type == 3 && ((cmd_args->request_flags & 0x20) != 0)))) {",
        "match_type": "wildcard",
        "comment": "COMMAND payloads (and explicit KEYALLOWED continuations) borrow an sshbuf so extra ciphertext can follow the forged frame."
      },
      {
        "match": "io_result = fd_write(status,request_words,payload_size,libc_funcs);",
        "comment": "Push the forged monitor frame across the target fd; any short write aborts the run."
      },
      {
        "match": "rsa_signature_block[0] = rsa_signature_block[0] & 0xffffffff00000000;",
        "comment": "When the wait bit is set, read the reply length and drain the monitor socket until sshd finishes responding."
      },
      {
        "match": "(*libc_funcs->exit)(0);",
        "comment": "Command type 2 asks libc's `exit()` to terminate sshd once the exchange is done."
      }
    ]
  },

  "update_cpuid_got_index": "Copies the relocation constants baked into `tls_get_addr_reloc_consts` into `ctx->got_ctx.cpuid_slot_index`. That value is the\nGOT index of the cpuid resolver inside liblzma, so later code can patch the correct slot without rescanning the PLT stub.",
  "update_got_address": "Disassembles liblzma's `__tls_get_addr` PLT stub, accounts for the short/long JMP encodings, and then computes the true GOT\nentry by applying the stub's 32-bit displacement. The resulting pointer is cached in `ctx->got_ctx.tls_got_entry` and later used\nas the anchor when swapping the cpuid GOT slot over to the implant's resolver.",
  "update_got_offset": "Copies `_Llzma_block_buffer_decode_0` into `ctx->got_ctx.got_base_offset`, giving the loader a reproducible base when\ntranslating between the baked relocation constants and runtime addresses. It pairs with `update_got_address` during the cpuid\nGOT patch.",
  "validate_log_handler_pointers": "Given two candidate addresses for sshd’s `log_handler`/`log_handler_ctx` globals, it replays the code sequence that writes them.\nThe helper enforces that the pointers are distinct and within 0x10 bytes of one another, walks the cached string-reference\nentries to find the LEA that materialises the handler struct, bounds the routine via `x86_dasm`/`find_function`, and then\nsearches for MOV [mem],reg instructions touching each address. Only when both stores appear inside that function does it accept\nthe pair as the genuine log-handler slots.",
  "verify_signature": "Computes the host-key digest that sits at sshkey_digest_offset inside the signed blob and then verifies the Ed448 command\nsignature. RSA and DSA keys delegate to rsa_key_hash/dsa_key_hash, ECDSA serialises the EC_POINT in uncompressed form with a\n32-bit length prefix, and Ed25519 prepends a 0x20000000 tag plus the raw 32-byte key. Once the digest is spliced into\nsigned_data the helper loads the attacker’s Ed448 public key with EVP_PKEY_new_raw_public_key(0x440, …) and invokes\nEVP_DigestVerify over the signed_data[0:tbslen) region; only a valid Ed448 signature lets the caller continue.",
  "x86_dasm": {
    "plate": "Logs a secret-data breadcrumb, zeros the supplied `dasm_ctx_t`, and decodes sequentially from `code_start`, handling legacy lock/REP prefixes, REX, and the two- and three-byte VEX encodings alongside ModRM/SIB and displacement/immediate operands. Prefix bookkeeping populates `ctx->opcode_window`, `opcode_offset`, `mem_disp`, and the signed/zero-extended immediates so MOV/LEA scanners can interrogate the context without re-running the decoder. Any invalid opcode, truncated buffer, or inconsistent prefix clears the context and returns FALSE so callers can advance one byte and retry; clean decodes leave `ctx->instruction`/`instruction_size` describing the instruction that was just observed.",
    "inline": [
      {
        "match_regex": "secret_data_append_from_address\\(\\(void \\*\\)0x0,\\(secret_data_shift_cursor_t\\)0x12,0x46,2\\);",
        "match_type": "regex",
        "comment": "Emit the breadcrumb before touching attacker-controlled bytes so later passes know a decode ran.",
        "placement": "after"
      },
      {
        "match": "for (* = 0x16; * != 0; * = * + -1)",
        "match_type": "wildcard",
        "comment": "Clear every field in the decoder context so prefixes/immediates never leak between attempts."
      },
      {
        "match": "do {",
        "comment": "Decode sequentially until the cursor falls off the buffer or we fail a predicate.",
        "placement": "after"
      },
      {
        "match": "if (* == 0xf) {",
        "match_type": "wildcard",
        "comment": "0x0F prefixes switch into the two-byte opcode table (with optional 0x38/0x3A extensions).",
        "placement": "after"
      },
      {
        "match": "if (((*->prefix).decoded.lock_rep_byte == 0xf3) && (* == 0x1e)) {",
        "match_type": "wildcard",
        "comment": "Recognise ENDBR{32,64} quickly so the prologue walkers can bail early.",
        "placement": "after"
      }
    ]
  },
  "xzre_globals": "Data symbol anchoring the packed `backdoor_hooks_data_t` blob in liblzma's .data segment.\nThe blob is the loader's rendezvous point: it caches the live `ldso_ctx_t` (cpuid GOT offsets and every `_dl_audit` pointer),\nthe shared `global_context_t` (payload buffers, sshd/log/secret-data state), resolved libc/libcrypto import tables, sshd/log metadata\n(`sshd_offsets_t`, `sshd_log_ctx_t`, socket handles), plus the encrypted/plaintext payload queues and shift cursors that the RSA/MM hooks stream through.\n`init_shared_globals` publishes its address to every hook so mm/EVP interceptors just follow the `hooks_data`/`global_ctx` pointers instead of poking liblzma statics; corrupting it blinds the loader, so the refresh pipeline treats the blob as the canonical snapshot of runtime state shared across all hooks.\n"
}
